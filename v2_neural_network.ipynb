{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b63b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ニューラルネットワーク版ダウンサンプリングバグ予測パイプライン ===\n",
      "データファイル 'method-p.csv' を読み込み中...\n",
      "データ読み込み完了。120167行を検出しました。\n",
      "利用可能な列: ['Project', 'Hash', 'LongName', 'Parent', 'CC', 'CCL', 'CCO', 'CI', 'CLC', 'CLLC', 'LDC', 'LLDC', 'HCPL', 'HDIF', 'HEFF', 'HNDB', 'HPL', 'HPV', 'HTRP', 'HVOL', 'MI', 'MIMS', 'MISEI', 'MISM', 'McCC', 'NL', 'NLE', 'NII', 'NOI', 'CD', 'CLOC', 'DLOC', 'TCD', 'TCLOC', 'LLOC', 'LOC', 'NOS', 'NUMPAR', 'TLLOC', 'TLOC', 'TNOS', 'WarningBlocker', 'WarningCritical', 'WarningInfo', 'WarningMajor', 'WarningMinor', 'Android Rules', 'Basic Rules', 'Brace Rules', 'Clone Implementation Rules', 'Code Size Rules', 'Comment Rules', 'Controversial Rules', 'Coupling Rules', 'Design Rules', 'Empty Code Rules', 'Finalizer Rules', 'Import Statement Rules', 'J2EE Rules', 'JUnit Rules', 'Jakarta Commons Logging Rules', 'Java Logging Rules', 'JavaBean Rules', 'MigratingToJUnit4 Rules', 'Migration Rules', 'Migration13 Rules', 'Migration14 Rules', 'Migration15 Rules', 'Naming Rules', 'Optimization Rules', 'Security Code Guideline Rules', 'Strict Exception Rules', 'String and StringBuffer Rules', 'Type Resolution Rules', 'Unnecessary and Unused Code Rules', 'Vulnerability Rules', 'Number of Bugs']\n",
      "データ準備中...\n",
      "使用する数値特徴量: ['CC', 'CCL', 'CCO', 'CI', 'CLC', 'CLLC', 'LDC', 'LLDC', 'HCPL', 'HDIF', 'HEFF', 'HNDB', 'HPL', 'HPV', 'HTRP', 'HVOL', 'MI', 'MIMS', 'MISEI', 'MISM', 'McCC', 'NL', 'NLE', 'NII', 'NOI', 'CD', 'CLOC', 'DLOC', 'TCD', 'TCLOC', 'LLOC', 'LOC', 'NOS', 'NUMPAR', 'TLLOC', 'TLOC', 'TNOS', 'WarningBlocker', 'WarningCritical', 'WarningInfo', 'WarningMajor', 'WarningMinor', 'Android Rules', 'Basic Rules', 'Brace Rules', 'Clone Implementation Rules', 'Code Size Rules', 'Comment Rules', 'Controversial Rules', 'Coupling Rules', 'Design Rules', 'Empty Code Rules', 'Finalizer Rules', 'Import Statement Rules', 'J2EE Rules', 'JUnit Rules', 'Jakarta Commons Logging Rules', 'Java Logging Rules', 'JavaBean Rules', 'MigratingToJUnit4 Rules', 'Migration Rules', 'Migration13 Rules', 'Migration14 Rules', 'Migration15 Rules', 'Naming Rules', 'Optimization Rules', 'Security Code Guideline Rules', 'Strict Exception Rules', 'String and StringBuffer Rules', 'Type Resolution Rules', 'Unnecessary and Unused Code Rules', 'Vulnerability Rules']\n",
      "数値特徴量の数: 72\n",
      "データ準備完了。特徴量数: 1587\n",
      "  - 数値特徴量: 72\n",
      "  - LongName TF-IDF: 1000\n",
      "  - Parent TF-IDF: 500\n",
      "  - Project One-Hot: 15\n",
      "ダウンサンプリング処理中...\n",
      "元のクラス分布: クラス0=82363, クラス1=37804\n",
      "クラス0を 82363 件から 37804 件にダウンサンプリングしました。\n",
      "ダウンサンプリング完了。新しいデータセットサイズ: 75608行\n",
      "新しいクラス分布: クラス0=37804、クラス1=37804\n",
      "ダウンサンプリング後データセットサイズ: 75608行\n",
      "訓練データ (ダウンサンプリング後): 60486行, テストデータ (ダウンサンプリング後): 15122行\n",
      "\n",
      "=== ニューラルネットワークでは特徴量重要度を直接計算しないため、このステップはスキップします。===\n",
      "特徴量選択は全特徴量を使用します。\n",
      "\n",
      "=== ニューラルネットワークでは特徴量削減をスキップし、全特徴量を使用します。 ===\n",
      "選択された特徴量数: 1587\n",
      "  - 数値特徴量: 72\n",
      "  - LongName TF-IDF: 1000\n",
      "  - Parent TF-IDF: 500\n",
      "  - Project One-Hot: 15\n",
      "\n",
      "=== ニューラルネットワークでは特徴量削減をスキップし、全特徴量を使用します。 ===\n",
      "選択された特徴量数: 1587\n",
      "  - 数値特徴量: 72\n",
      "  - LongName TF-IDF: 1000\n",
      "  - Parent TF-IDF: 500\n",
      "  - Project One-Hot: 15\n",
      "\n",
      "=== Log Lossベース ベイジアン最適化（ニューラルネットワーク使用）===\n",
      "最適化手法: Bayesian Optimization (scikit-optimize)\n",
      "探索パラメータ: nn_hidden_layers, nn_neurons, nn_learning_rate, dropout_rate\n",
      "クラス不均衡対応: ダウンサンプリング (事前に適用済み)\n",
      "特徴量: 数値 + Java TF-IDF + One-Hot Encoding + 正規化\n",
      "Bayesian Optimization開始...\n",
      "新しい最良損失: 0.6618\n",
      "パラメータ: {'nn_hidden_layers': 3, 'nn_neurons': 73, 'nn_learning_rate': 0.00362561763457623, 'dropout_rate': 0.33874006317859484}\n",
      "新しい最良損失: 0.6339\n",
      "パラメータ: {'nn_hidden_layers': 2, 'nn_neurons': 54, 'nn_learning_rate': 0.0008288916866885144, 'dropout_rate': 0.23348344445560876}\n",
      "新しい最良損失: 0.6211\n",
      "パラメータ: {'nn_hidden_layers': 1, 'nn_neurons': 178, 'nn_learning_rate': 0.00012966511753760403, 'dropout_rate': 0.38879950890673}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay,\n",
    "                           roc_curve, auc, precision_recall_curve,\n",
    "                           log_loss, accuracy_score, f1_score,\n",
    "                           precision_score, recall_score)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real\n",
    "import warnings\n",
    "import re\n",
    "from typing import List, Set\n",
    "\n",
    "# --- Keras/TensorFlowのインポート ---\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 再現性のためのseed固定\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "tf.random.set_seed(GLOBAL_SEED) # TensorFlowのseedも固定\n",
    "\n",
    "class JavaCodeTokenizer:\n",
    "    \"\"\"\n",
    "    Javaのメソッド名やクラス名を適切にトークン化するカスタムトークナイザー\n",
    "\n",
    "    主な機能:\n",
    "    1. パッケージ名、クラス名、メソッド名の分離\n",
    "    2. CamelCaseの分割\n",
    "    3. アンダースコア区切りの分割\n",
    "    4. 特殊文字の処理\n",
    "    5. ストップワードの除去\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_token_length: int = 2, include_package_tokens: bool = True):\n",
    "        \"\"\"\n",
    "        初期化\n",
    "\n",
    "        Parameters:\n",
    "            min_token_length (int): 最小トークン長（これより短いトークンは除外）\n",
    "            include_package_tokens (bool): パッケージ名のトークンを含めるかどうかのフラグ\n",
    "        \"\"\"\n",
    "        self.min_token_length = min_token_length\n",
    "        self.include_package_tokens = include_package_tokens\n",
    "        # 汎用的なJavaのストップワードリスト（適宜追加・調整してください）\n",
    "        self.java_stopwords = self._get_java_stopwords()\n",
    "\n",
    "    def _get_java_stopwords(self) -> Set[str]:\n",
    "        \"\"\"Javaコードに特有のストップワードを定義\"\"\"\n",
    "        # 一般的なキーワード、型、アクセス修飾子、予約語など\n",
    "        return {\n",
    "            \"public\", \"private\", \"protected\", \"class\", \"interface\", \"enum\",\n",
    "            \"void\", \"int\", \"long\", \"double\", \"float\", \"boolean\", \"char\", \"byte\", \"short\",\n",
    "            \"String\", \"Object\", \"System\", \"out\", \"println\", \"new\", \"this\", \"super\",\n",
    "            \"return\", \"if\", \"else\", \"for\", \"while\", \"do\", \"switch\", \"case\", \"default\",\n",
    "            \"break\", \"continue\", \"try\", \"catch\", \"finally\", \"throw\", \"throws\",\n",
    "            \"import\", \"package\", \"static\", \"final\", \"abstract\", \"transient\", \"volatile\",\n",
    "            \"synchronized\", \"native\", \"strictfp\", \"extends\", \"implements\", \"instanceof\",\n",
    "            \"assert\", \"const\", \"goto\", \"null\", \"true\", \"false\",\n",
    "            \"get\", \"set\", # getter/setterによく使われる\n",
    "            \"to\", \"of\", \"in\", \"on\", \"with\", \"from\", \"and\", \"or\", \"not\", \"is\",\n",
    "            \"var\", \"let\", \"const\", # 新しいJavaバージョンでのキーワード\n",
    "            \"util\", \"io\", \"lang\", \"math\", \"net\", \"sql\", \"awt\", \"swing\", # 標準ライブラリの一部\n",
    "            \"value\", \"data\", \"info\", \"mgr\", \"impl\", \"factory\", \"builder\", \"adapter\",\n",
    "            \"handler\", \"controller\", \"service\", \"dao\", \"repository\", \"entity\", \"dto\",\n",
    "            \"model\", \"view\", \"component\", \"config\", \"init\", \"destroy\", \"main\",\n",
    "            \"list\", \"map\", \"set\", \"array\", \"collection\", \"stream\", \"buffer\",\n",
    "            \"reader\", \"writer\", \"input\", \"output\", \"exception\", \"error\", \"event\",\n",
    "            \"create\", \"update\", \"delete\", \"remove\", \"add\", \"find\", \"load\", \"save\",\n",
    "            \"run\", \"execute\", \"perform\", \"process\", \"handle\", \"check\", \"validate\",\n",
    "            \"parse\", \"format\", \"convert\", \"build\", \"generate\", \"initiate\", \"start\", \"stop\"\n",
    "        }\n",
    "\n",
    "    def tokenize(self, code_element_name: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Javaのコード要素名（メソッド名、クラス名など）をトークン化する。\n",
    "        \"\"\"\n",
    "        if not isinstance(code_element_name, str):\n",
    "            return []\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        # まず '.' で分割し、パッケージ名と実際の名前を分離\n",
    "        parts = code_element_name.split('.')\n",
    "        if self.include_package_tokens:\n",
    "            # パッケージ名もトークンとして含める場合\n",
    "            for part in parts[:-1]: # 最後の要素（クラス/メソッド名）以外はパッケージ名\n",
    "                tokens.extend(self._split_camel_case_and_underscore(part))\n",
    "\n",
    "        # 最後の要素（クラス名またはメソッド名）を処理\n",
    "        final_name = parts[-1]\n",
    "\n",
    "        # カンマ以降（例: \"method(arg1,arg2)\"）を削除して名前のみにする\n",
    "        final_name = final_name.split('(')[0]\n",
    "\n",
    "        # 記号を除去し、CamelCaseとアンダースコアで分割\n",
    "        cleaned_name = re.sub(r'[^a-zA-Z0-9_]', '', final_name)\n",
    "        split_tokens = self._split_camel_case_and_underscore(cleaned_name)\n",
    "        tokens.extend(split_tokens)\n",
    "\n",
    "        # ストップワード除去と最小トークン長フィルタリング\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            lower_token = token.lower()\n",
    "            if lower_token not in self.java_stopwords and len(lower_token) >= self.min_token_length:\n",
    "                filtered_tokens.append(lower_token)\n",
    "\n",
    "        return filtered_tokens\n",
    "\n",
    "    def _split_camel_case_and_underscore(self, name: str) -> List[str]:\n",
    "        \"\"\"CamelCaseとアンダースコアで文字列を分割するヘルパーメソッド\"\"\"\n",
    "        # アンダースコアで分割\n",
    "        words = name.split('_')\n",
    "        split_words = []\n",
    "        for word in words:\n",
    "            # CamelCaseで分割\n",
    "            camel_split = re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?=[A-Z]|$)', word)\n",
    "            split_words.extend([s for s in camel_split if s]) # 空文字列を除外\n",
    "\n",
    "        return split_words\n",
    "\n",
    "\n",
    "class SimplifiedBugHunter:\n",
    "    \"\"\"\n",
    "    カスタムJavaトークナイザー統合版ダウンサンプリング版バグ予測クラス\n",
    "    （ニューラルネットワーク版）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_selection_threshold: float = 0.001,\n",
    "                 tfidf_max_features: int = 500,\n",
    "                 java_tokenizer_min_length: int = 2,\n",
    "                 include_package_tokens: bool = False,\n",
    "                 # NNのための新しいハイパーパラメータのデフォルト値\n",
    "                 nn_epochs: int = 100,       # エポック数 (EarlyStoppingで調整)\n",
    "                 nn_batch_size: int = 32):  # バッチサイズ\n",
    "\n",
    "        self.best_model = None\n",
    "        self.feature_importance = None # NNでは直接重要度を出すのは難しい\n",
    "        self.all_feature_names = None\n",
    "        self.selected_features = None # NN版では通常、全特徴量を選択する\n",
    "        self.optimization_history = []\n",
    "        self.best_params = None\n",
    "        self.best_loss = float('inf')\n",
    "        self.feature_selection_threshold = feature_selection_threshold\n",
    "        self.initial_X = None\n",
    "        self.initial_y = None\n",
    "        self.tfidf_vectorizer_longname = None\n",
    "        self.tfidf_vectorizer_parent = None\n",
    "        self.tfidf_max_features = tfidf_max_features\n",
    "        self.project_dummies_columns = None\n",
    "        self.scaler = None\n",
    "\n",
    "        self.java_tokenizer = JavaCodeTokenizer(\n",
    "            min_token_length=java_tokenizer_min_length,\n",
    "            include_package_tokens=include_package_tokens\n",
    "        )\n",
    "\n",
    "        self.downsampled_X = None\n",
    "        self.downsampled_y = None\n",
    "        self.original_class_distribution = None\n",
    "        self.downsampled_train_distribution = None\n",
    "\n",
    "        # NNのデフォルトハイパーパラメータを保存\n",
    "        self.nn_epochs = nn_epochs\n",
    "        self.nn_batch_size = nn_batch_size\n",
    "\n",
    "    def read_data(self, data_path: str) -> pd.DataFrame:\n",
    "        \"\"\"データを読み込む\"\"\"\n",
    "        print(f\"データファイル '{data_path}' を読み込み中...\")\n",
    "        data = pd.read_csv(data_path)\n",
    "        print(f\"データ読み込み完了。{len(data)}行を検出しました。\")\n",
    "        print(f\"利用可能な列: {list(data.columns)}\")\n",
    "        return data\n",
    "\n",
    "    def prepare_data(self, data: pd.DataFrame, is_training: bool = True) -> tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        データを前処理する。\n",
    "        数値特徴量、TF-IDF特徴量、One-Hot Encoding特徴量を生成し、結合する。\n",
    "        訓練時のみスケーラーやTF-IDFベクトライザーを学習する。\n",
    "        \"\"\"\n",
    "        print(\"データ準備中...\")\n",
    "\n",
    "        # --- 修正箇所: ターゲット変数を 'Number of Bugs' に変更し、二値化 ---\n",
    "        # 'Number of Bugs' をターゲット変数 y に設定し、0は0、1以上は1に変換\n",
    "        y = (data['Number of Bugs'] > 0).astype(int).copy()\n",
    "        # 'Number of Bugs' カラムを特徴量 X から削除\n",
    "        X = data.drop(columns=['Number of Bugs']).copy()\n",
    "\n",
    "        # 初期データを保存（トークナイザー分析用）\n",
    "        if is_training:\n",
    "            self.initial_X = X.copy()\n",
    "            self.initial_y = y.copy()\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        # CSVファイルに実際に存在する数値特徴量を動的に特定\n",
    "        # 文字列系のカラムを除外して、数値系のカラムのみを選択\n",
    "        exclude_columns = ['Project', 'Hash', 'LongName', 'Parent']  # 文字列カラム\n",
    "        numerical_feature_columns = []\n",
    "\n",
    "        for col in X.columns:\n",
    "            if col not in exclude_columns:\n",
    "                # 数値型に変換可能かチェック\n",
    "                try:\n",
    "                    pd.to_numeric(X[col], errors='raise')\n",
    "                    numerical_feature_columns.append(col)\n",
    "                except (ValueError, TypeError):\n",
    "                    # 数値に変換できない場合は除外\n",
    "                    print(f\"警告: {col} は数値特徴量として使用できません。スキップします。\")\n",
    "                    continue\n",
    "\n",
    "        print(f\"使用する数値特徴量: {numerical_feature_columns}\")\n",
    "        print(f\"数値特徴量の数: {len(numerical_feature_columns)}\")\n",
    "\n",
    "        # 数値特徴量の処理\n",
    "        if numerical_feature_columns:\n",
    "            X_numerical = X[numerical_feature_columns].copy()\n",
    "            # 数値型に変換\n",
    "            for col in numerical_feature_columns:\n",
    "                X_numerical[col] = pd.to_numeric(X_numerical[col], errors='coerce')\n",
    "\n",
    "            X_numerical = X_numerical.replace([np.inf, -np.inf], np.nan)\n",
    "            X_numerical = X_numerical.fillna(0) # NaNを0で埋める\n",
    "\n",
    "            if is_training:\n",
    "                self.scaler = StandardScaler()\n",
    "                X_numerical_scaled = self.scaler.fit_transform(X_numerical)\n",
    "            else:\n",
    "                if self.scaler is None:\n",
    "                    raise ValueError(\"Scalerが学習されていません。訓練フェーズを先に実行してください。\")\n",
    "                X_numerical_scaled = self.scaler.transform(X_numerical)\n",
    "\n",
    "            X_numerical_scaled_df = pd.DataFrame(X_numerical_scaled, columns=numerical_feature_columns, index=X.index)\n",
    "        else:\n",
    "            # 数値特徴量がない場合は空のDataFrameを作成\n",
    "            X_numerical_scaled_df = pd.DataFrame(index=X.index)\n",
    "\n",
    "        # TF-IDF特徴量の処理（LongName）\n",
    "        if 'LongName' in X.columns:\n",
    "            longname_data = X['LongName'].fillna(\"\").astype(str)\n",
    "            longname_tokenized = longname_data.apply(self.java_tokenizer.tokenize).apply(lambda x: \" \".join(x))\n",
    "\n",
    "            if is_training:\n",
    "                self.tfidf_vectorizer_longname = TfidfVectorizer(max_features=self.tfidf_max_features)\n",
    "                X_longname_tfidf = self.tfidf_vectorizer_longname.fit_transform(longname_tokenized)\n",
    "            else:\n",
    "                if self.tfidf_vectorizer_longname is None:\n",
    "                    raise ValueError(\"LongName TF-IDF Vectorizerが学習されていません。訓練フェーズを先に実行してください。\")\n",
    "                X_longname_tfidf = self.tfidf_vectorizer_longname.transform(longname_tokenized)\n",
    "\n",
    "            X_longname_tfidf_df = pd.DataFrame(X_longname_tfidf.toarray(),\n",
    "                                               columns=[f'LongName_tfidf_{i}' for i in range(X_longname_tfidf.shape[1])],\n",
    "                                               index=X.index)\n",
    "        else:\n",
    "            X_longname_tfidf_df = pd.DataFrame(index=X.index)\n",
    "            print(\"警告: 'LongName'カラムが見つかりません。TF-IDF特徴量をスキップします。\")\n",
    "\n",
    "        # TF-IDF特徴量の処理（Parent）\n",
    "        if 'Parent' in X.columns:\n",
    "            parent_data = X['Parent'].fillna(\"\").astype(str)\n",
    "            parent_tokenized = parent_data.apply(self.java_tokenizer.tokenize).apply(lambda x: \" \".join(x))\n",
    "\n",
    "            if is_training:\n",
    "                self.tfidf_vectorizer_parent = TfidfVectorizer(max_features=self.tfidf_max_features // 2) # Parentは少し少なく設定\n",
    "                X_parent_tfidf = self.tfidf_vectorizer_parent.fit_transform(parent_tokenized)\n",
    "            else:\n",
    "                if self.tfidf_vectorizer_parent is None:\n",
    "                    raise ValueError(\"Parent TF-IDF Vectorizerが学習されていません。訓練フェーズを先に実行してください。\")\n",
    "                X_parent_tfidf = self.tfidf_vectorizer_parent.transform(parent_tokenized)\n",
    "\n",
    "            X_parent_tfidf_df = pd.DataFrame(X_parent_tfidf.toarray(),\n",
    "                                             columns=[f'Parent_tfidf_{i}' for i in range(X_parent_tfidf.shape[1])],\n",
    "                                             index=X.index)\n",
    "        else:\n",
    "            X_parent_tfidf_df = pd.DataFrame(index=X.index)\n",
    "            print(\"警告: 'Parent'カラムが見つかりません。Parent TF-IDF特徴量をスキップします。\")\n",
    "\n",
    "        # One-Hot Encoding（Project）\n",
    "        if 'Project' in X.columns:\n",
    "            project_data = X['Project'].fillna(\"Unknown\").astype(str)\n",
    "            if is_training:\n",
    "                X_project_onehot = pd.get_dummies(project_data, prefix='Project', dtype=int)\n",
    "                # 訓練データからダミー変数のカラム名を保存\n",
    "                self.project_dummies_columns = X_project_onehot.columns\n",
    "            else:\n",
    "                X_project_onehot = pd.get_dummies(project_data, prefix='Project', dtype=int)\n",
    "                # 訓練時に保存したカラムに再インデックス化し、存在しないプロジェクトは0で埋める\n",
    "                X_project_onehot = X_project_onehot.reindex(columns=self.project_dummies_columns, fill_value=0)\n",
    "        else:\n",
    "            X_project_onehot = pd.DataFrame(index=X.index)\n",
    "            print(\"警告: 'Project'カラムが見つかりません。One-Hot Encoding特徴量をスキップします。\")\n",
    "\n",
    "        # 全特徴量を結合\n",
    "        X_processed = pd.concat([X_numerical_scaled_df, X_longname_tfidf_df, X_parent_tfidf_df, X_project_onehot], axis=1)\n",
    "\n",
    "        self.all_feature_names = X_processed.columns.tolist()\n",
    "\n",
    "        print(f\"データ準備完了。特徴量数: {len(self.all_feature_names)}\")\n",
    "        print(f\"  - 数値特徴量: {len(numerical_feature_columns)}\")\n",
    "        print(f\"  - LongName TF-IDF: {X_longname_tfidf_df.shape[1] if not X_longname_tfidf_df.empty else 0}\")\n",
    "        print(f\"  - Parent TF-IDF: {X_parent_tfidf_df.shape[1] if not X_parent_tfidf_df.empty else 0}\")\n",
    "        print(f\"  - Project One-Hot: {X_project_onehot.shape[1] if not X_project_onehot.empty else 0}\")\n",
    "\n",
    "        return X_processed, y\n",
    "\n",
    "    def apply_downsampling(self, X: pd.DataFrame, y: pd.Series) -> tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        少数派クラスのサンプル数を多数派クラスに合わせるためにダウンサンプリングを適用する。\n",
    "        \"\"\"\n",
    "        print(\"ダウンサンプリング処理中...\")\n",
    "\n",
    "        # 元のクラス分布を保存\n",
    "        class_0_original = y[y == 0].count()\n",
    "        class_1_original = y[y == 1].count()\n",
    "        self.original_class_distribution = {\n",
    "            'class_0': class_0_original,\n",
    "            'class_1': class_1_original,\n",
    "            'total': len(y)\n",
    "        }\n",
    "\n",
    "        print(f\"元のクラス分布: クラス0={class_0_original}, クラス1={class_1_original}\")\n",
    "\n",
    "        # クラス0とクラス1のインデックスを分離\n",
    "        X_class_0 = X[y == 0]\n",
    "        y_class_0 = y[y == 0]\n",
    "        X_class_1 = X[y == 1]\n",
    "        y_class_1 = y[y == 1]\n",
    "\n",
    "        # 多数派クラス (通常はクラス0) のサンプル数を少数派クラス (通常はクラス1) に合わせる\n",
    "        # ここでは、常にクラス1の数にクラス0をダウンサンプリング\n",
    "        n_samples_class_1 = len(X_class_1)\n",
    "\n",
    "        if n_samples_class_1 == 0:\n",
    "            print(\"警告: 少数派クラス (バグあり) のサンプルがありません。ダウンサンプリングをスキップします。\")\n",
    "            self.downsampled_train_distribution = self.original_class_distribution\n",
    "            return X, y\n",
    "\n",
    "        if len(X_class_0) > n_samples_class_1:\n",
    "            X_class_0_downsampled, y_class_0_downsampled = resample(\n",
    "                X_class_0, y_class_0,\n",
    "                replace=False,    # 非復元抽出\n",
    "                n_samples=n_samples_class_1, # 少数派クラスの数に合わせる\n",
    "                random_state=GLOBAL_SEED # 再現性のためのシード固定\n",
    "            )\n",
    "            print(f\"クラス0を {len(X_class_0)} 件から {len(X_class_0_downsampled)} 件にダウンサンプリングしました。\")\n",
    "        else:\n",
    "            X_class_0_downsampled = X_class_0\n",
    "            y_class_0_downsampled = y_class_0\n",
    "            print(\"クラス0のサンプル数が少数派クラス以下なので、ダウンサンプリングは不要です。\")\n",
    "\n",
    "        # ダウンサンプリングされた多数派クラスと少数派クラスを結合\n",
    "        X_downsampled = pd.concat([X_class_0_downsampled, X_class_1])\n",
    "        y_downsampled = pd.concat([y_class_0_downsampled, y_class_1])\n",
    "\n",
    "        # シャッフル (重要: クラスの偏りをなくすため)\n",
    "        shuffled_indices = np.random.permutation(len(X_downsampled))\n",
    "        self.downsampled_X = X_downsampled.iloc[shuffled_indices].reset_index(drop=True)\n",
    "        self.downsampled_y = y_downsampled.iloc[shuffled_indices].reset_index(drop=True)\n",
    "\n",
    "        # ダウンサンプリング後の訓練データ分布を保存\n",
    "        self.downsampled_train_distribution = {\n",
    "            'class_0': self.downsampled_y[self.downsampled_y == 0].count(),\n",
    "            'class_1': self.downsampled_y[self.downsampled_y == 1].count(),\n",
    "            'total': len(self.downsampled_y)\n",
    "        }\n",
    "\n",
    "        print(f\"ダウンサンプリング完了。新しいデータセットサイズ: {len(self.downsampled_y)}行\")\n",
    "        print(f\"新しいクラス分布: クラス0={self.downsampled_train_distribution['class_0']}、クラス1={self.downsampled_train_distribution['class_1']}\")\n",
    "\n",
    "        return self.downsampled_X, self.downsampled_y\n",
    "\n",
    "    def _build_nn_model(self, input_dim: int, params: dict) -> keras.Model:\n",
    "        \"\"\"ニューラルネットワークモデルの構築\"\"\"\n",
    "        model = Sequential()\n",
    "\n",
    "        # 隠れ層の数 (最小1層)\n",
    "        num_hidden_layers = int(params.get('nn_hidden_layers', 1))\n",
    "        # 各隠れ層のニューロン数\n",
    "        neurons = int(params.get('nn_neurons', 64))\n",
    "        # ドロップアウト率\n",
    "        dropout_rate = params.get('dropout_rate', 0.2) # デフォルト値を設定\n",
    "\n",
    "        # 最初の隠れ層\n",
    "        model.add(Dense(neurons, activation='relu', input_shape=(input_dim,)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "        # 残りの隠れ層\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            model.add(Dense(neurons, activation='relu'))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "        # 出力層 (二値分類なのでsigmoid)\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # モデルのコンパイル\n",
    "        optimizer = Adam(learning_rate=params.get('nn_learning_rate', 0.001))\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='binary_crossentropy', # 二値分類の標準的な損失関数\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def evaluate_model_with_cv(self, params: dict, X: pd.DataFrame, y: pd.Series,\n",
    "                              k_folds: int = 3) -> float:\n",
    "        \"\"\"交差検証を用いたモデル評価（ニューラルネットワーク版）\"\"\"\n",
    "        try:\n",
    "            # XをNumPy配列に変換 (NNの入力形式に合わせる)\n",
    "            X_np = X.to_numpy()\n",
    "            y_np = y.to_numpy()\n",
    "\n",
    "            cv = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=GLOBAL_SEED)\n",
    "            total_loss = 0.0\n",
    "\n",
    "            # EarlyStoppingコールバックの設定 (過学習防止)\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "            for train_idx, val_idx in cv.split(X_np, y_np):\n",
    "                X_train_fold, X_val_fold = X_np[train_idx], X_np[val_idx]\n",
    "                y_train_fold, y_val_fold = y_np[train_idx], y_np[val_idx]\n",
    "\n",
    "                # モデル構築\n",
    "                model = self._build_nn_model(X_train_fold.shape[1], params)\n",
    "\n",
    "                # モデル学習\n",
    "                history = model.fit(X_train_fold, y_train_fold,\n",
    "                                    epochs=self.nn_epochs, # nn_epochsは__init__で設定したデフォルト値を使用\n",
    "                                    batch_size=self.nn_batch_size, # nn_batch_sizeは__init__で設定したデフォルト値を使用\n",
    "                                    validation_data=(X_val_fold, y_val_fold),\n",
    "                                    callbacks=[early_stopping], # EarlyStoppingを適用\n",
    "                                    verbose=0) # 学習中の出力を抑制\n",
    "\n",
    "                # 最良エポックのval_lossを取得\n",
    "                fold_loss = min(history.history['val_loss'])\n",
    "                total_loss += fold_loss\n",
    "\n",
    "            avg_loss = total_loss / k_folds\n",
    "            return avg_loss\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"評価エラー: {e}\")\n",
    "            return float('inf')\n",
    "\n",
    "\n",
    "    def optimize_hyperparameters_with_log_loss(self, X: pd.DataFrame, y: pd.Series,\n",
    "                                                max_iterations: int = 30) -> dict:\n",
    "        \"\"\"Log Loss損失関数を用いたベイジアン最適化（ニューラルネットワーク版）\"\"\"\n",
    "        print(\"\\n=== Log Lossベース ベイジアン最適化（ニューラルネットワーク使用）===\")\n",
    "        print(\"最適化手法: Bayesian Optimization (scikit-optimize)\")\n",
    "        print(\"探索パラメータ: nn_hidden_layers, nn_neurons, nn_learning_rate, dropout_rate\")\n",
    "        print(\"クラス不均衡対応: ダウンサンプリング (事前に適用済み)\")\n",
    "        print(\"特徴量: 数値 + Java TF-IDF + One-Hot Encoding + 正規化\")\n",
    "\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_params = None\n",
    "        self.optimization_history = []\n",
    "\n",
    "        # 探索空間の定義 (NN用)\n",
    "        search_space = [\n",
    "            Integer(1, 3, name='nn_hidden_layers'), # 隠れ層の数: 1から3層\n",
    "            Integer(32, 256, name='nn_neurons'),    # 各隠れ層のニューロン数: 32から256\n",
    "            Real(1e-4, 1e-2, prior='log-uniform', name='nn_learning_rate'), # 学習率: 0.0001から0.01 (対数均等に)\n",
    "            Real(0.1, 0.5, name='dropout_rate') # ドロップアウト率: 0.1から0.5\n",
    "        ]\n",
    "\n",
    "        # 目的関数\n",
    "        def objective(params):\n",
    "            nn_hidden_layers, nn_neurons, nn_learning_rate, dropout_rate = params\n",
    "\n",
    "            param_dict = {\n",
    "                'nn_hidden_layers': int(nn_hidden_layers),\n",
    "                'nn_neurons': int(nn_neurons),\n",
    "                'nn_learning_rate': float(nn_learning_rate),\n",
    "                'dropout_rate': float(dropout_rate),\n",
    "            }\n",
    "\n",
    "            # モデル評価\n",
    "            loss = self.evaluate_model_with_cv(param_dict, X, y)\n",
    "\n",
    "            # 履歴記録\n",
    "            self.optimization_history.append({\n",
    "                'params': param_dict.copy(),\n",
    "                'loss': loss\n",
    "            })\n",
    "\n",
    "            # 最良パラメータ更新\n",
    "            if loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "                self.best_params = param_dict.copy()\n",
    "                print(f\"新しい最良損失: {loss:.4f}\")\n",
    "                print(f\"パラメータ: {param_dict}\")\n",
    "\n",
    "            return loss\n",
    "\n",
    "        # Bayesian Optimization実行\n",
    "        print(\"Bayesian Optimization開始...\")\n",
    "        result = gp_minimize(\n",
    "            func=objective,\n",
    "            dimensions=search_space,\n",
    "            n_calls=max_iterations,\n",
    "            random_state=GLOBAL_SEED,\n",
    "            acq_func='EI',\n",
    "            n_initial_points=5\n",
    "        )\n",
    "\n",
    "        print(f\"\\nBayesian Optimization完了!\")\n",
    "        print(f\"最良損失: {self.best_loss:.4f}\")\n",
    "        final_best_params = self.best_params.copy() if self.best_params else {}\n",
    "        print(f\"最良パラメータ: {final_best_params}\")\n",
    "        print(f\"総評価回数: {len(self.optimization_history)}\")\n",
    "\n",
    "        return final_best_params\n",
    "\n",
    "    def train_initial_model_for_feature_importance(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"\n",
    "        ニューラルネットワークでは特徴量重要度を直接計算することが難しい。\n",
    "        ここでは、特徴量選択のために初期モデルを学習するステップをスキップする。\n",
    "        \"\"\"\n",
    "        print(\"\\n=== ニューラルネットワークでは特徴量重要度を直接計算しないため、このステップはスキップします。===\")\n",
    "        self.feature_importance = None # 明示的にNoneに設定\n",
    "        # selected_featuresを全特徴量に設定\n",
    "        self.selected_features = X.columns.tolist()\n",
    "        print(\"特徴量選択は全特徴量を使用します。\")\n",
    "\n",
    "\n",
    "    def select_features_by_importance(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ニューラルネットワークでは、このステップはそのままでは適用できない。\n",
    "        今回は簡略化のため、全特徴量を選択することにする。\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== ニューラルネットワークでは特徴量削減をスキップし、全特徴量を使用します。 ===\")\n",
    "\n",
    "        # train_initial_model_for_feature_importanceで設定されているはず\n",
    "        if self.selected_features is None:\n",
    "            self.selected_features = X.columns.tolist()\n",
    "\n",
    "        X_selected = X[self.selected_features]\n",
    "\n",
    "        print(f\"選択された特徴量数: {len(self.selected_features)}\")\n",
    "\n",
    "        # 特徴量タイプ別の統計 (これはそのまま使える)\n",
    "        longname_count = len([f for f in self.selected_features if f.startswith('LongName_tfidf_')])\n",
    "        parent_count = len([f for f in self.selected_features if f.startswith('Parent_tfidf_')])\n",
    "        project_count = len([f for f in self.selected_features if f.startswith('Project_')])\n",
    "        numerical_count = len(self.selected_features) - longname_count - parent_count - project_count\n",
    "\n",
    "        print(f\"  - 数値特徴量: {numerical_count}\")\n",
    "        print(f\"  - LongName TF-IDF: {longname_count}\")\n",
    "        print(f\"  - Parent TF-IDF: {parent_count}\")\n",
    "        print(f\"  - Project One-Hot: {project_count}\")\n",
    "\n",
    "        return X_selected\n",
    "\n",
    "\n",
    "    def train_optimized_model(self, X: pd.DataFrame, y: pd.Series, optimal_params: dict):\n",
    "        \"\"\"最適化されたパラメータでニューラルネットワークモデルを学習\"\"\"\n",
    "        print(\"\\n=== 最適化ニューラルネットワークモデル学習（カスタムJavaトークナイザー版）===\")\n",
    "\n",
    "        # X, y をNumPy配列に変換\n",
    "        X_np = X.to_numpy()\n",
    "        y_np = y.to_numpy()\n",
    "\n",
    "        # モデル構築\n",
    "        model = self._build_nn_model(X_np.shape[1], optimal_params)\n",
    "\n",
    "        # EarlyStoppingコールバックの設定 (val_lossが5エポック改善しなかったら停止)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        # モデル学習\n",
    "        print(f\"学習パラメータ: Epochs={self.nn_epochs}, Batch Size={self.nn_batch_size}\")\n",
    "        print(f\"NN構造: 隠れ層={optimal_params.get('nn_hidden_layers')}, ニューロン数={optimal_params.get('nn_neurons')}\")\n",
    "        print(f\"学習率={optimal_params.get('nn_learning_rate'):.6f}, ドロップアウト率={optimal_params.get('dropout_rate'):.2f}\")\n",
    "\n",
    "\n",
    "        history = model.fit(X_np, y_np,\n",
    "                            epochs=self.nn_epochs,\n",
    "                            batch_size=self.nn_batch_size,\n",
    "                            validation_split=0.2, # 訓練データの一部をバリデーションに使用\n",
    "                            callbacks=[early_stopping],\n",
    "                            verbose=1) # 学習中の出力を表示\n",
    "\n",
    "        self.best_model = model\n",
    "\n",
    "        print(\"最適化モデル学習完了\")\n",
    "        # ニューラルネットワークではfeature_importanceを直接取得できないためNoneのまま\n",
    "        self.feature_importance = None\n",
    "        return self.best_model\n",
    "\n",
    "    def comprehensive_evaluation(self, X_test: pd.DataFrame, y_test: pd.Series) -> dict:\n",
    "        \"\"\"包括的評価（デフォルトしきい値0.5のみ使用）\"\"\"\n",
    "        print(\"\\n=== 包括的評価 ===\")\n",
    "\n",
    "        # X_testをNumPy配列に変換\n",
    "        X_test_np = X_test.to_numpy()\n",
    "\n",
    "        # 予測確率取得\n",
    "        y_pred_proba = self.best_model.predict(X_test_np).flatten()\n",
    "\n",
    "        # デフォルトしきい値（0.5）での予測\n",
    "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "        # 評価指標の計算\n",
    "        results = {\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "            'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_test, y_pred),\n",
    "            'LogLoss': log_loss(y_test, y_pred_proba), # Kerasのlog_lossではなくsklearnのlog_lossを使用\n",
    "            'Threshold': 0.5\n",
    "        }\n",
    "\n",
    "        print(\"\\n--- 評価結果（しきい値=0.5）---\")\n",
    "        for metric, score in results.items():\n",
    "            if metric != 'Threshold':\n",
    "                print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "        return results, y_pred, y_pred_proba\n",
    "\n",
    "    def plot_results(self, results: dict, y_test: pd.Series, y_pred: np.ndarray,\n",
    "                    y_pred_proba: np.ndarray, X_test: pd.DataFrame):\n",
    "        \"\"\"結果の可視化（ニューラルネットワーク版、特徴量重要度表示は調整）\"\"\"\n",
    "        print(\"\\n=== 結果可視化 ===\")\n",
    "\n",
    "        plt.figure(figsize=(18, 15)) # 全体的なサイズを調整\n",
    "\n",
    "        # 1. 混同行列\n",
    "        plt.subplot(3, 3, 1)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        display_matrix = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,\n",
    "                                              display_labels=[0, 1])\n",
    "        display_matrix.plot(ax=plt.gca(), cmap='Blues')\n",
    "        plt.title(f\"混同行列（しきい値=0.5）\")\n",
    "\n",
    "        # 2. ROC曲線\n",
    "        plt.subplot(3, 3, 2)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "                label=f'ROC曲線 (AUC = {roc_auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('偽陽性率')\n",
    "        plt.ylabel('真陽性率')\n",
    "        plt.title('ROC曲線')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        # 3. Precision-Recall曲線\n",
    "        plt.subplot(3, 3, 3)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        plt.plot(recall, precision, color='blue', lw=2)\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall曲線')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 4. 評価指標表示\n",
    "        plt.subplot(3, 3, 4)\n",
    "        metrics = ['Accuracy', 'F1', 'Precision', 'Recall', 'LogLoss']\n",
    "        values = [results.get(metric, 0) for metric in metrics]\n",
    "\n",
    "        bars = plt.bar(metrics, values, color='lightblue', alpha=0.8)\n",
    "        plt.title('評価指標')\n",
    "        plt.ylabel('スコア')\n",
    "        plt.ylim(0, 1.0) # LogLossは1以上になることもあるので注意が必要だが、一般的には小さい方が良い\n",
    "        # LogLossの値が極端に大きい場合を考慮してy軸の範囲を調整\n",
    "        max_logloss = max(values) if 'LogLoss' in metrics else 0.0\n",
    "        plt.ylim(0, max(1.0, max_logloss * 1.1)) # 最小0、最大で一番大きい値の1.1倍\n",
    "\n",
    "        for bar, value in zip(bars, values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                    f'{value:.3f}' if value <=1.0 else f'{value:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "\n",
    "        # 5. 特徴量重要度（NNでは直接的な重要度がないため、表示を調整）\n",
    "        plt.subplot(3, 3, 5)\n",
    "        plt.text(0.5, 0.5, 'ニューラルネットワークでは\\n直接的な特徴量重要度は\\n計算されません',\n",
    "                 horizontalalignment='center', verticalalignment='center',\n",
    "                 transform=plt.gca().transAxes, fontsize=12, color='gray')\n",
    "        plt.title('特徴量重要度 (NN)')\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "        # 6. ベイジアン最適化履歴\n",
    "        plt.subplot(3, 3, 6)\n",
    "        if self.optimization_history:\n",
    "            losses = [entry['loss'] for entry in self.optimization_history]\n",
    "            iterations = range(len(losses))\n",
    "\n",
    "            plt.plot(iterations, losses, 'bo-', linewidth=2, markersize=4,\n",
    "                    alpha=0.7, label='評価点')\n",
    "\n",
    "            # 最良値の推移\n",
    "            cumulative_best = []\n",
    "            current_best = float('inf')\n",
    "            for loss in losses:\n",
    "                if loss < current_best:\n",
    "                    current_best = loss\n",
    "                cumulative_best.append(current_best)\n",
    "\n",
    "            plt.plot(iterations, cumulative_best, 'r-', linewidth=3,\n",
    "                    label='最良値推移')\n",
    "\n",
    "            plt.xlabel('評価回数')\n",
    "            plt.ylabel('Log Loss値')\n",
    "            plt.title('ベイジアン最適化過程')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "\n",
    "        # 7. ダウンサンプリング効果の可視化\n",
    "        plt.subplot(3, 3, 7)\n",
    "        if self.original_class_distribution and self.downsampled_train_distribution:\n",
    "            original_counts_total = [self.original_class_distribution['class_0'],\n",
    "                                     self.original_class_distribution['class_1']]\n",
    "            downsampled_counts_train = [self.downsampled_train_distribution['class_0'],\n",
    "                                        self.downsampled_train_distribution['class_1']]\n",
    "\n",
    "            x = np.arange(2)\n",
    "            width = 0.35\n",
    "\n",
    "            plt.bar(x - width/2, original_counts_total, width, label='元データ全体', alpha=0.7, color='lightcoral')\n",
    "            plt.bar(x + width/2, downsampled_counts_train, width, label='ダウンサンプリング後訓練データ', alpha=0.7, color='lightblue')\n",
    "\n",
    "            plt.xlabel('クラス')\n",
    "            plt.ylabel('サンプル数')\n",
    "            plt.title('ダウンサンプリング効果')\n",
    "            plt.xticks(x, ['クラス 0', 'クラス 1'])\n",
    "            plt.legend()\n",
    "\n",
    "            for i, (orig, down) in enumerate(zip(original_counts_total, downsampled_counts_train)):\n",
    "                plt.text(i - width/2, orig + max(original_counts_total) * 0.01, f'{orig}',\n",
    "                        ha='center', va='bottom', fontsize=9)\n",
    "                plt.text(i + width/2, down + max(original_counts_total) * 0.01, f'{down}',\n",
    "                        ha='center', va='bottom', fontsize=9)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'ダウンサンプリング情報なし',\n",
    "                     horizontalalignment='center', verticalalignment='center',\n",
    "                     transform=plt.gca().transAxes, fontsize=12, color='gray')\n",
    "            plt.title('ダウンサンプリング効果')\n",
    "            plt.axis('off')\n",
    "\n",
    "        # 8. 予測確率分布\n",
    "        plt.subplot(3, 3, 8)\n",
    "        plt.hist(y_pred_proba[y_test == 0], bins=30, alpha=0.7, label='実際のクラス 0', color='blue')\n",
    "        plt.hist(y_pred_proba[y_test == 1], bins=30, alpha=0.7, label='実際のクラス 1', color='red')\n",
    "        plt.axvline(x=0.5, color='gray', linestyle='--', label='しきい値 0.5')\n",
    "        plt.xlabel('予測確率')\n",
    "        plt.ylabel('頻度')\n",
    "        plt.title('予測確率分布')\n",
    "        plt.legend()\n",
    "\n",
    "        # 9. サマリー表示\n",
    "        plt.subplot(3, 3, 9)\n",
    "        plt.axis('off')\n",
    "\n",
    "        feature_count = len(self.selected_features) if self.selected_features else 0\n",
    "\n",
    "        if self.original_class_distribution and self.downsampled_train_distribution:\n",
    "            reduction_rate = (1 - self.downsampled_train_distribution['total'] / self.original_class_distribution['total']) * 100\n",
    "            downsample_info = f\"ダウンサンプリング: {reduction_rate:.1f}%削減\"\n",
    "        else:\n",
    "            downsample_info = \"ダウンサンプリング: 情報なし\"\n",
    "\n",
    "        if self.selected_features:\n",
    "            longname_count = len([f for f in self.selected_features if f.startswith('LongName_tfidf_')])\n",
    "            parent_count = len([f for f in self.selected_features if f.startswith('Parent_tfidf_')])\n",
    "            project_count = len([f for f in self.selected_features if f.startswith('Project_')])\n",
    "            numerical_count = feature_count - longname_count - parent_count - project_count\n",
    "        else:\n",
    "            longname_count = parent_count = project_count = numerical_count = 0\n",
    "\n",
    "        # NNのハイパーパラメータを表示に追加\n",
    "        nn_params = results.get('optimal_params', {})\n",
    "        summary_text = f\"\"\"\n",
    "ニューラルネットワーク版バグ予測結果\n",
    "\n",
    "使用特徴量数: {feature_count}個\n",
    "  数値: {numerical_count}, LongName TF-IDF: {longname_count}\n",
    "  Parent TF-IDF: {parent_count}, Project: {project_count}\n",
    "TF-IDF最大特徴量数: {self.tfidf_max_features}\n",
    "{downsample_info}\n",
    "使用しきい値: 0.500 (固定)\n",
    "\n",
    "最適化NNパラメータ:\n",
    "  隠れ層: {self.best_params.get('nn_hidden_layers', 'N/A')}\n",
    "  ニューロン: {self.best_params.get('nn_neurons', 'N/A')}\n",
    "  学習率: {self.best_params.get('nn_learning_rate', 'N/A'):.6f}\n",
    "  ドロップアウト率: {self.best_params.get('dropout_rate', 'N/A'):.2f}\n",
    "  バッチサイズ: {self.nn_batch_size} (固定)\n",
    "  最大エポック: {self.nn_epochs} (EarlyStopping適用)\n",
    "\n",
    "評価結果（テストデータ）:\n",
    "F1スコア: {results['F1']:.3f}\n",
    "Precision: {results['Precision']:.3f}\n",
    "Recall: {results['Recall']:.3f}\n",
    "Accuracy: {results['Accuracy']:.3f}\n",
    "Log Loss: {results['LogLoss']:.3f}\n",
    "\n",
    "ニューラルネットワークでJavaコードの\n",
    "特徴を学習し、バグを予測\n",
    "        \"\"\"\n",
    "        plt.text(0.1, 0.5, summary_text, fontsize=8,\n",
    "                verticalalignment='center', fontfamily='monospace')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def run_pipeline(self, data_path: str):\n",
    "        \"\"\"ニューラルネットワークを組み込んだパイプラインの実行\"\"\"\n",
    "        print(\"=== ニューラルネットワーク版ダウンサンプリングバグ予測パイプライン ===\")\n",
    "\n",
    "        # 1. データ読み込み\n",
    "        data = self.read_data(data_path)\n",
    "\n",
    "        # 2. データ準備（カスタムJavaトークナイザー、TF-IDF特徴量、One-Hot Encoding特徴量、正規化を使用）\n",
    "        X_full, y_full = self.prepare_data(data, is_training=True)\n",
    "\n",
    "        # 3. ダウンサンプリングをここで一度だけ適用\n",
    "        X_downsampled, y_downsampled = self.apply_downsampling(X_full, y_full)\n",
    "        print(f\"ダウンサンプリング後データセットサイズ: {len(X_downsampled)}行\")\n",
    "\n",
    "        # 4. データ分割（ダウンサンプリング後のデータに対して）\n",
    "        X_train_ds, X_test_ds, y_train_ds, y_test_ds = train_test_split(\n",
    "            X_downsampled, y_downsampled, test_size=0.2, random_state=GLOBAL_SEED, stratify=y_downsampled\n",
    "        )\n",
    "        print(f\"訓練データ (ダウンサンプリング後): {len(X_train_ds)}行, テストデータ (ダウンサンプリング後): {len(X_test_ds)}行\")\n",
    "\n",
    "        # 5. 特徴量重要度取得のための初期モデル学習（NNではこのステップをスキップ）\n",
    "        self.train_initial_model_for_feature_importance(X_train_ds, y_train_ds)\n",
    "\n",
    "\n",
    "        # 6. 特徴量削減（NN版では全特徴量を使用するように変更）\n",
    "        # self.selected_featuresがこの時点でセットされているはず\n",
    "        X_train_reduced = self.select_features_by_importance(X_train_ds)\n",
    "        X_test_reduced = self.select_features_by_importance(X_test_ds)\n",
    "\n",
    "\n",
    "        # 7. Log Lossベースハイパーパラメータ最適化（削減された訓練データで実施）\n",
    "        optimal_params = self.optimize_hyperparameters_with_log_loss(\n",
    "            X_train_reduced, y_train_ds, max_iterations=30\n",
    "        )\n",
    "\n",
    "        # 最適化されたパラメータをクラスインスタンスに保存\n",
    "        self.best_params = optimal_params\n",
    "\n",
    "        # 8. 最適化モデル学習（削減された訓練データで実施）\n",
    "        optimized_model = self.train_optimized_model(\n",
    "            X_train_reduced, y_train_ds, optimal_params\n",
    "        )\n",
    "\n",
    "        # 9. 評価（削減されたテストデータで実施）\n",
    "        results, y_pred, y_pred_proba = self.comprehensive_evaluation(\n",
    "            X_test_reduced, y_test_ds\n",
    "        )\n",
    "        # 結果に最適なパラメータを追加 (プロットのサマリー表示用)\n",
    "        results['optimal_params'] = optimal_params\n",
    "\n",
    "        # 10. 結果可視化\n",
    "        self.plot_results(\n",
    "            results, y_test_ds, y_pred, y_pred_proba, X_test_reduced\n",
    "        )\n",
    "\n",
    "        return results, optimal_params\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> tuple:\n",
    "        \"\"\"予測（ニューラルネットワーク版）\"\"\"\n",
    "        if self.best_model is None:\n",
    "            raise ValueError(\"モデルが学習されていません。まずrun_pipeline()を実行してください。\")\n",
    "        if self.selected_features is None:\n",
    "            raise ValueError(\"特徴量削減が実行されていません。まずrun_pipeline()を実行してください。\")\n",
    "        if self.tfidf_vectorizer_longname is None or self.tfidf_vectorizer_parent is None:\n",
    "            raise ValueError(\"TF-IDF Vectorizerが学習されていません。まずrun_pipeline()を実行してください。\")\n",
    "        if self.project_dummies_columns is None:\n",
    "            raise ValueError(\"One-Hot Encoderが学習されていません。まずrun_pipeline()を実行してください。\")\n",
    "        if self.scaler is None:\n",
    "            raise ValueError(\"Scalerが学習されていません。まずrun_pipeline()を実行してください。\")\n",
    "\n",
    "        # データ準備（推論時はis_training=False）\n",
    "        X_processed_full, _ = self.prepare_data(X, is_training=False)\n",
    "\n",
    "        # 学習時に選択された特徴量のみを選択 (NN版では全特徴量を選択する前提)\n",
    "        # prepare_dataで既に全特徴量を処理済みなので、ここでは単に整合性を保つ\n",
    "        if self.selected_features and len(self.selected_features) > 0:\n",
    "            X_processed = X_processed_full[self.selected_features]\n",
    "        else:\n",
    "            # selected_featuresが空またはNoneの場合、全特徴量を使用\n",
    "            X_processed = X_processed_full\n",
    "            print(\"警告: selected_featuresが設定されていないため、全ての処理済み特徴量を使用します。\")\n",
    "\n",
    "        # NumPy配列に変換して予測\n",
    "        y_pred_proba = self.best_model.predict(X_processed.to_numpy()).flatten()\n",
    "\n",
    "        # デフォルトしきい値での予測\n",
    "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "        return y_pred, y_pred_proba\n",
    "\n",
    "    def get_feature_analysis(self) -> dict:\n",
    "        \"\"\"特徴量分析結果の取得（NN版に合わせて調整）\"\"\"\n",
    "        params_to_return = self.best_params.copy() if self.best_params else {}\n",
    "\n",
    "        downsampling_info = {}\n",
    "        if self.original_class_distribution and self.downsampled_train_distribution:\n",
    "            downsampling_info = {\n",
    "                'original_class_0': self.original_class_distribution['class_0'],\n",
    "                'original_class_1': self.original_class_distribution['class_1'],\n",
    "                'original_total': self.original_class_distribution['total'],\n",
    "                'downsampled_train_class_0': self.downsampled_train_distribution['class_0'],\n",
    "                'downsampled_train_class_1': self.downsampled_train_distribution['class_1'],\n",
    "                'downsampled_train_total': self.downsampled_train_distribution['total'],\n",
    "                'reduction_rate': (1 - self.downsampled_train_distribution['total'] / self.original_class_distribution['total']) * 100\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'best_params': params_to_return,\n",
    "            'optimization_history': self.optimization_history,\n",
    "            'feature_importance': None, # NNでは直接取得できない\n",
    "            'selected_features': self.selected_features,\n",
    "            'all_feature_names': self.all_feature_names,\n",
    "            'feature_selection_threshold': self.feature_selection_threshold,\n",
    "            'tfidf_max_features': self.tfidf_max_features,\n",
    "            'downsampling_info': downsampling_info,\n",
    "            'java_tokenizer_settings': {\n",
    "                'min_token_length': self.java_tokenizer.min_token_length,\n",
    "                'include_package_tokens': self.java_tokenizer.include_package_tokens,\n",
    "                'stopwords_count': len(self.java_tokenizer.java_stopwords)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def display_feature_importance_table(self, top_n: int = 10):\n",
    "        \"\"\"特徴量重要度テーブルの表示（NN版に合わせて調整）\"\"\"\n",
    "        print(\"\\n=== ニューラルネットワークでは直接的な特徴量重要度テーブルは表示できません。===\")\n",
    "        print(\"他の手法（例：SHAP値など）を用いて、特徴量の影響を分析できます。\")\n",
    "\n",
    "    def display_downsampling_summary(self):\n",
    "        \"\"\"ダウンサンプリング結果のサマリーを表示する\"\"\"\n",
    "        print(\"\\n=== ダウンサンプリングサマリー ===\")\n",
    "        if self.original_class_distribution and self.downsampled_train_distribution:\n",
    "            orig_0 = self.original_class_distribution['class_0']\n",
    "            orig_1 = self.original_class_distribution['class_1']\n",
    "            orig_total = self.original_class_distribution['total']\n",
    "            down_0 = self.downsampled_train_distribution['class_0']\n",
    "            down_1 = self.downsampled_train_distribution['class_1']\n",
    "            down_total = self.downsampled_train_distribution['total']\n",
    "\n",
    "            print(f\"元データ合計: {orig_total} (クラス0: {orig_0}, クラス1: {orig_1})\")\n",
    "            print(f\"ダウンサンプリング後訓練データ合計: {down_total} (クラス0: {down_0}, クラス1: {down_1})\")\n",
    "\n",
    "            if orig_total > 0:\n",
    "                reduction_rate = (1 - down_total / orig_total) * 100\n",
    "                print(f\"データ削減率: {reduction_rate:.1f}%\")\n",
    "            else:\n",
    "                print(\"元のデータが空です。\")\n",
    "        else:\n",
    "            print(\"ダウンサンプリング情報はまだ利用できません。\")\n",
    "\n",
    "    def display_tokenizer_analysis(self, sample_size: int = 3):\n",
    "        \"\"\"トークナイザーの動作例と分析を表示する\"\"\"\n",
    "        print(\"\\n=== Javaコードトークナイザー分析 ===\")\n",
    "        print(f\"最小トークン長: {self.java_tokenizer.min_token_length}\")\n",
    "        print(f\"パッケージ名トークンを含める: {self.java_tokenizer.include_package_tokens}\")\n",
    "        print(f\"ストップワード数: {len(self.java_tokenizer.java_stopwords)}\")\n",
    "\n",
    "        # データが読み込まれているか確認\n",
    "        if self.initial_X is None:\n",
    "            print(\"データがまだ読み込まれていないため、トークナイザーの動作例を表示できません。\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n--- サンプルメソッド名（{sample_size}件）とトークン化結果 ---\")\n",
    "        sample_methods = self.initial_X['LongName'].fillna(\"\").astype(str).sample(min(sample_size, len(self.initial_X)), random_state=GLOBAL_SEED)\n",
    "\n",
    "        for i, method_name in enumerate(sample_methods):\n",
    "            tokens = self.java_tokenizer.tokenize(method_name)\n",
    "            print(f\"  [{i+1}]\")\n",
    "            print(f\"    元メソッド名: {method_name}\")\n",
    "            print(f\"    トークン化結果: {tokens}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "        print(f\"\\n--- サンプル親要素名（{sample_size}件）とトークン化結果 ---\")\n",
    "        sample_parents = self.initial_X['Parent'].fillna(\"\").astype(str).sample(min(sample_size, len(self.initial_X)), random_state=GLOBAL_SEED)\n",
    "\n",
    "        for i, parent_name in enumerate(sample_parents):\n",
    "            tokens = self.java_tokenizer.tokenize(parent_name)\n",
    "            print(f\"  [{i+1}]\")\n",
    "            print(f\"    元親要素名: {parent_name}\")\n",
    "            print(f\"    トークン化結果: {tokens}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "# 使用例 (if __name__ == \"__main__\": ブロック)\n",
    "if __name__ == \"__main__\":\n",
    "    # CSVファイルパスを指定\n",
    "    data_path = \"method-p.csv\"\n",
    "\n",
    "    # カスタムJavaトークナイザー統合版バグハンターのインスタンス作成\n",
    "    bug_hunter = SimplifiedBugHunter(\n",
    "        feature_selection_threshold=0.001, # NN版ではあまり意味をなさないが、引数として残しておく\n",
    "        tfidf_max_features=500,\n",
    "        java_tokenizer_min_length=2,\n",
    "        include_package_tokens=False, # パッケージ名を除外して、より重要な部分に集中\n",
    "        # ニューラルネットワークのハイパーパラメータの固定値（最適化対象外）\n",
    "        nn_epochs=100, # EarlyStoppingで早期終了するので、大きめに設定\n",
    "        nn_batch_size=64\n",
    "    )\n",
    "\n",
    "    # パイプライン実行\n",
    "    results, optimal_params = bug_hunter.run_pipeline(data_path)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ニューラルネットワーク版バグ予測完了!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"F1スコア: {results['F1']:.3f}\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    print(f\"Log Loss: {results['LogLoss']:.3f}\")\n",
    "\n",
    "\n",
    "    # ダウンサンプリングサマリーの表示\n",
    "    bug_hunter.display_downsampling_summary()\n",
    "\n",
    "    # 特徴量重要度テーブルの表示 (NN版ではスキップされることを明示)\n",
    "    bug_hunter.display_feature_importance_table(top_n=15)\n",
    "\n",
    "    # トークナイザーの動作例表示\n",
    "    bug_hunter.display_tokenizer_analysis(sample_size=3)\n",
    "\n",
    "    feature_analysis = bug_hunter.get_feature_analysis()\n",
    "    print(f\"\\n最適パラメータ (NN): {feature_analysis['best_params']}\")\n",
    "    print(f\"選択された特徴量数: {len(feature_analysis['selected_features'])}\") # NN版では全特徴量\n",
    "    print(f\"全特徴量数 (Java TF-IDF含む): {len(feature_analysis['all_feature_names'])}\")\n",
    "    print(f\"TF-IDF最大特徴量数: {feature_analysis['tfidf_max_features']}\")\n",
    "    print(f\"Javaトークナイザー設定: {feature_analysis['java_tokenizer_settings']}\")\n",
    "\n",
    "    if feature_analysis['downsampling_info']:\n",
    "        ds_info = feature_analysis['downsampling_info']\n",
    "        print(f\"データ削減率: {ds_info['reduction_rate']:.1f}%\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bug-hunter-FzxI4xcd-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
