{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3810ef35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== カスタムJavaトークナイザー版ダウンサンプリングバグ予測パイプライン ===\n",
      "\n",
      "=== 1) データ読み込み ===\n",
      "読み込み完了: 120167行, 77列\n",
      "\n",
      "=== データ前処理（カスタムJavaトークナイザー + TF-IDF + One-Hot Encoding + 正規化使用）===\n",
      "LongName TF-IDF特徴量数: 1000\n",
      "Parent TF-IDF特徴量数: 1000\n",
      "初期使用特徴量数 (数値 + Java TF-IDF + One-Hot Encoding): 2087\n",
      "  - 数値特徴量: 72\n",
      "  - LongName TF-IDF: 1000\n",
      "  - Parent TF-IDF: 1000\n",
      "  - Project One-Hot: 15\n",
      "ラベル分布: 0=82363, 1=37804\n",
      "クラス分布: 0.315 (1の割合)\n",
      "\n",
      "=== サンプルトークン化結果 ===\n",
      "LongName例: org.elasticsearch.index.fielddata.plain.GeoPointDoubleArrayAtomicFieldData$Empty.<init>()V\n",
      "→ トークン: ['geo', 'point', 'array', 'atomic', 'field', 'data', 'constructor']\n",
      "Parent例: org.elasticsearch.index.fielddata.plain.GeoPointDoubleArrayAtomicFieldData$Empty\n",
      "→ トークン: ['geo', 'point', 'array', 'atomic', 'field', 'data']\n",
      "\n",
      "=== ダウンサンプリング適用 ===\n",
      "ダウンサンプリング前 - クラス0: 82363件, クラス1: 37804件\n",
      "ダウンサンプリング後 - クラス0: 37804件, クラス1: 37804件\n",
      "総データ数: 75608件 (削減率: 37.1%)\n",
      "ダウンサンプリング後データセットサイズ: 75608行\n",
      "訓練データ (ダウンサンプリング後): 60486行, テストデータ (ダウンサンプリング後): 15122行\n",
      "\n",
      "=== 特徴量重要度取得のための初期モデル学習（カスタムJavaトークナイザー版）===\n",
      "初期モデル学習と特徴量重要度の計算が完了しました。\n",
      "\n",
      "=== 特徴量削減 ===\n",
      "特徴量削減前の数: 2087\n",
      "選択された特徴量数 (閾値 0.001): 97\n",
      "  - 数値特徴量: 46\n",
      "  - LongName TF-IDF: 24\n",
      "  - Parent TF-IDF: 20\n",
      "  - Project One-Hot: 7\n",
      "\n",
      "=== 特徴量削減 ===\n",
      "特徴量削減前の数: 2087\n",
      "選択された特徴量数 (閾値 0.001): 97\n",
      "  - 数値特徴量: 46\n",
      "  - LongName TF-IDF: 24\n",
      "  - Parent TF-IDF: 20\n",
      "  - Project One-Hot: 7\n",
      "\n",
      "=== Log Lossベース ベイジアン最適化（カスタムJavaトークナイザー使用）===\n",
      "最適化手法: Bayesian Optimization (scikit-optimize)\n",
      "探索パラメータ: n_estimators, max_depth\n",
      "クラス不均衡対応: ダウンサンプリング (事前に適用済み)\n",
      "特徴量: 数値 + Java TF-IDF + One-Hot Encoding + 正規化\n",
      "Bayesian Optimization開始...\n",
      "新しい最良損失: 0.5967\n",
      "パラメータ: {'n_estimators': 259, 'max_depth': 12}\n",
      "新しい最良損失: 0.5743\n",
      "パラメータ: {'n_estimators': 256, 'max_depth': 16}\n",
      "新しい最良損失: 0.5699\n",
      "パラメータ: {'n_estimators': 129, 'max_depth': 17}\n",
      "新しい最良損失: 0.5577\n",
      "パラメータ: {'n_estimators': 100, 'max_depth': 20}\n",
      "新しい最良損失: 0.5564\n",
      "パラメータ: {'n_estimators': 300, 'max_depth': 20}\n",
      "\n",
      "Bayesian Optimization完了!\n",
      "最良損失: 0.5564\n",
      "最良パラメータ: {'n_estimators': 300, 'max_depth': 20}\n",
      "総評価回数: 30\n",
      "\n",
      "=== 最適化モデル学習（カスタムJavaトークナイザー版）===\n",
      "最適化モデル学習完了\n",
      "最終パラメータ: {'n_estimators': 300, 'max_depth': 20, 'random_state': 42, 'n_jobs': -1}\n",
      "学習データ: 60486件\n",
      "学習済みモデルから特徴量重要度を計算しました。\n",
      "\n",
      "=== 包括的評価 ===\n",
      "\n",
      "--- 評価結果（しきい値=0.5）---\n",
      "Accuracy: 0.7067\n",
      "F1: 0.7335\n",
      "Precision: 0.6720\n",
      "Recall: 0.8074\n",
      "LogLoss: 0.5406\n",
      "\n",
      "============================================================\n",
      "カスタムJavaトークナイザー版バグ予測完了!\n",
      "============================================================\n",
      "F1スコア: 0.734\n",
      "Precision: 0.672\n",
      "Recall: 0.807\n",
      "Accuracy: 0.707\n",
      "\n",
      "=== ダウンサンプリング サマリー ===\n",
      "元データ (全データ):\n",
      "  クラス 0: 82,363件\n",
      "  クラス 1: 37,804件\n",
      "  合計: 120,167件\n",
      "\n",
      "ダウンサンプリング後 (訓練データ):\n",
      "  クラス 0: 37,804件\n",
      "  クラス 1: 37,804件\n",
      "  合計: 75,608件\n",
      "\n",
      "元データ全体からの削減率: 37.1%\n",
      "クラス不均衡比 (クラス0/クラス1):\n",
      "  元データ (全データ): 2.18:1\n",
      "  ダウンサンプリング後 (訓練データ): 1.00:1\n",
      "\n",
      "=== 上位15特徴量重要度 (カスタムJavaトークナイザー版) ===\n",
      "  特徴量 タイプ      重要度\n",
      " HVOL  数値 0.047911\n",
      " HCPL  数値 0.045817\n",
      " HTRP  数値 0.041268\n",
      " HNDB  数値 0.040592\n",
      " MIMS  数値 0.039255\n",
      "  HPL  数値 0.038355\n",
      " HEFF  数値 0.037700\n",
      "   MI  数値 0.037220\n",
      "  HPV  数値 0.035630\n",
      " HDIF  数値 0.034414\n",
      "MISEI  数値 0.034055\n",
      " MISM  数値 0.033669\n",
      "  NOI  数値 0.029689\n",
      "  NII  数値 0.027120\n",
      " TLOC  数値 0.024625\n",
      "\n",
      "=== 特徴量タイプ別統計 ===\n",
      "数値: 46個 (平均重要度: 0.0174)\n",
      "LongName TF-IDF: 24個 (平均重要度: 0.0036)\n",
      "Parent TF-IDF: 20個 (平均重要度: 0.0036)\n",
      "Project: 7個 (平均重要度: 0.0056)\n",
      "\n",
      "=== Javaトークナイザー動作例 ===\n",
      "設定:\n",
      "  最小トークン長: 2\n",
      "  パッケージトークン含む: False\n",
      "  ストップワード数: 47\n",
      "\n",
      "=== LongName トークン化例 (上位3件) ===\n",
      "1. org.elasticsearch.index.fielddata.plain.GeoPointDoubleArrayAtomicFieldData$Empty.<init>()V\n",
      "   → ['geo', 'point', 'array', 'atomic', 'field', 'data', 'constructor']\n",
      "\n",
      "2. com.thinkaurelius.titan.graphdb.idmanagement.IDPoolTest$441498#.supportsInterruption()Z\n",
      "   → ['pool', 'test', '441498#', 'supports', 'interruption']\n",
      "\n",
      "3. org.neo4j.kernel.impl.locking.ReadOnlyLocks.accept(Lorg/neo4j/kernel/impl/locking/Locks$Visitor;)V\n",
      "   → ['read', 'only', 'locks', 'accept']\n",
      "\n",
      "=== Parent トークン化例 (上位3件) ===\n",
      "1. org.elasticsearch.index.fielddata.plain.GeoPointDoubleArrayAtomicFieldData$Empty\n",
      "   → ['geo', 'point', 'array', 'atomic', 'field', 'data']\n",
      "\n",
      "2. com.thinkaurelius.titan.graphdb.idmanagement.IDPoolTest$441498#\n",
      "   → ['pool', 'test', '441498#']\n",
      "\n",
      "3. org.neo4j.kernel.impl.locking.ReadOnlyLocks\n",
      "   → ['read', 'only', 'locks']\n",
      "\n",
      "最適パラメータ: {'n_estimators': 300, 'max_depth': 20}\n",
      "選択された特徴量数: 97\n",
      "全特徴量数 (Java TF-IDF含む): 2087\n",
      "TF-IDF最大特徴量数: 1000\n",
      "Javaトークナイザー設定: {'min_token_length': 2, 'include_package_tokens': False, 'stopwords_count': 47}\n",
      "データ削減率: 37.1%\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay,\n",
    "                           roc_curve, auc, precision_recall_curve,\n",
    "                           log_loss, accuracy_score, f1_score,\n",
    "                           precision_score, recall_score)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample  # ダウンサンプリング用\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "import warnings\n",
    "import re\n",
    "from typing import List, Set\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 再現性のためのseed固定\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "\n",
    "class JavaCodeTokenizer:\n",
    "    \"\"\"\n",
    "    Javaのメソッド名やクラス名を適切にトークン化するカスタムトークナイザー\n",
    "\n",
    "    主な機能:\n",
    "    1. パッケージ名、クラス名、メソッド名の分離\n",
    "    2. CamelCaseの分割\n",
    "    3. アンダースコア区切りの分割\n",
    "    4. 特殊文字の処理\n",
    "    5. ストップワードの除去\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_token_length: int = 2, include_package_tokens: bool = True):\n",
    "        \"\"\"\n",
    "        初期化\n",
    "\n",
    "        Parameters:\n",
    "            min_token_length (int): 最小トークン長（これより短いトークンは除外）\n",
    "            include_package_tokens (bool): パッケージ名のトークンを含めるかどうか\n",
    "        \"\"\"\n",
    "        self.min_token_length = min_token_length\n",
    "        self.include_package_tokens = include_package_tokens\n",
    "\n",
    "        # Javaでよく使われる一般的な単語（ストップワード）\n",
    "        self.java_stopwords: Set[str] = {\n",
    "            'java', 'util', 'lang', 'io', 'net', 'org', 'com', 'javax',\n",
    "            'get', 'set', 'is', 'has', 'to', 'from', 'with', 'without',\n",
    "            'init', 'new', 'create', 'build', 'make', 'do', 'run', 'execute',\n",
    "            'class', 'interface', 'abstract', 'final', 'static', 'public',\n",
    "            'private', 'protected', 'void', 'int', 'string', 'boolean',\n",
    "            'double', 'float', 'long', 'short', 'byte', 'char',\n",
    "            'impl', 'default', 'base', 'simple', 'empty'\n",
    "        }\n",
    "\n",
    "    def _split_camel_case(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        CamelCaseの文字列を単語に分割\n",
    "\n",
    "        例: \"GeoPointDoubleArray\" -> [\"Geo\", \"Point\", \"Double\", \"Array\"]\n",
    "        \"\"\"\n",
    "        # 大文字の前で分割（先頭は除く）\n",
    "        parts = re.sub(r'(?<!^)(?=[A-Z])', ' ', text).split()\n",
    "        return [part for part in parts if len(part) >= self.min_token_length]\n",
    "\n",
    "    def _split_snake_case(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        snake_caseやkebab-caseの文字列を単語に分割\n",
    "\n",
    "        例: \"atomic_field_data\" -> [\"atomic\", \"field\", \"data\"]\n",
    "        \"\"\"\n",
    "        parts = re.split(r'[_\\-]', text)\n",
    "        return [part for part in parts if len(part) >= self.min_token_length]\n",
    "\n",
    "    def _extract_method_signature_tokens(self, method_signature: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        メソッドシグネチャからトークンを抽出\n",
    "\n",
    "        例: \"org.elasticsearch.index.fielddata.plain.GeoPointDoubleArrayAtomicFieldData$Empty.<init>()V\"\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "\n",
    "        # メソッドシグネチャのパターンマッチング\n",
    "        # パッケージ.クラス名.メソッド名(引数)戻り値型 の形式\n",
    "        pattern = r'^(.*?)\\.([^.()]+)\\(([^)]*)\\)(.*)$'\n",
    "        match = re.match(pattern, method_signature)\n",
    "\n",
    "        if match:\n",
    "            package_and_class = match.group(1)  # パッケージ+クラス部分\n",
    "            method_name = match.group(2)        # メソッド名\n",
    "            parameters = match.group(3)         # 引数部分\n",
    "            return_type = match.group(4)        # 戻り値型\n",
    "\n",
    "            # パッケージとクラス名の処理\n",
    "            if package_and_class:\n",
    "                package_parts = package_and_class.split('.')\n",
    "\n",
    "                # パッケージ名の処理（オプション）\n",
    "                if self.include_package_tokens:\n",
    "                    for part in package_parts[:-1]:  # 最後はクラス名なので除外\n",
    "                        if part and len(part) >= self.min_token_length:\n",
    "                            tokens.append(part.lower())\n",
    "\n",
    "                # クラス名の処理（最後の部分）\n",
    "                if package_parts:\n",
    "                    class_name = package_parts[-1]\n",
    "                    # 内部クラス記号 $ の処理\n",
    "                    class_parts = class_name.split('$')\n",
    "                    for class_part in class_parts:\n",
    "                        if class_part:\n",
    "                            # CamelCaseの分割\n",
    "                            camel_tokens = self._split_camel_case(class_part)\n",
    "                            tokens.extend([token.lower() for token in camel_tokens])\n",
    "\n",
    "            # メソッド名の処理\n",
    "            if method_name and method_name != '<init>' and method_name != '<clinit>':\n",
    "                # アンダースコアで分割\n",
    "                snake_tokens = self._split_snake_case(method_name)\n",
    "                for token in snake_tokens:\n",
    "                    # CamelCaseの分割\n",
    "                    camel_tokens = self._split_camel_case(token)\n",
    "                    tokens.extend([token.lower() for token in camel_tokens])\n",
    "            elif method_name in ['<init>', '<clinit>']:\n",
    "                tokens.append('constructor')\n",
    "\n",
    "        else:\n",
    "            # パターンにマッチしない場合は単純に分割\n",
    "            # ドット区切りで分割\n",
    "            parts = method_signature.split('.')\n",
    "            for part in parts:\n",
    "                if part and '(' not in part:  # 引数部分を除外\n",
    "                    # CamelCaseとsnake_caseの両方で分割\n",
    "                    snake_tokens = self._split_snake_case(part)\n",
    "                    for token in snake_tokens:\n",
    "                        camel_tokens = self._split_camel_case(token)\n",
    "                        tokens.extend([token.lower() for token in camel_tokens])\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _extract_class_name_tokens(self, class_name: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        クラス名からトークンを抽出\n",
    "\n",
    "        例: \"org.elasticsearch.index.fielddata.plain.GeoPointDoubleArrayAtomicFieldData$Empty\"\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "\n",
    "        # パッケージとクラス名を分離\n",
    "        parts = class_name.split('.')\n",
    "\n",
    "        # パッケージ名の処理（オプション）\n",
    "        if self.include_package_tokens:\n",
    "            for part in parts[:-1]:  # 最後はクラス名なので除外\n",
    "                if part and len(part) >= self.min_token_length:\n",
    "                    tokens.append(part.lower())\n",
    "\n",
    "        # クラス名の処理（最後の部分）\n",
    "        if parts:\n",
    "            class_part = parts[-1]\n",
    "            # 内部クラス記号 $ の処理\n",
    "            class_components = class_part.split('$')\n",
    "            for component in class_components:\n",
    "                if component:\n",
    "                    # CamelCaseの分割\n",
    "                    camel_tokens = self._split_camel_case(component)\n",
    "                    tokens.extend([token.lower() for token in camel_tokens])\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def tokenize_longname(self, longname: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        LongName列の値をトークン化（メソッドシグネチャ）\n",
    "        \"\"\"\n",
    "        if not longname or pd.isna(longname):\n",
    "            return []\n",
    "\n",
    "        tokens = self._extract_method_signature_tokens(str(longname))\n",
    "\n",
    "        # ストップワードの除去と最小長チェック\n",
    "        filtered_tokens = [\n",
    "            token for token in tokens\n",
    "            if len(token) >= self.min_token_length and token.lower() not in self.java_stopwords\n",
    "        ]\n",
    "\n",
    "        return filtered_tokens\n",
    "\n",
    "    def tokenize_parent(self, parent: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Parent列の値をトークン化（クラス名）\n",
    "        \"\"\"\n",
    "        if not parent or pd.isna(parent):\n",
    "            return []\n",
    "\n",
    "        tokens = self._extract_class_name_tokens(str(parent))\n",
    "\n",
    "        # ストップワードの除去と最小長チェック\n",
    "        filtered_tokens = [\n",
    "            token for token in tokens\n",
    "            if len(token) >= self.min_token_length and token.lower() not in self.java_stopwords\n",
    "        ]\n",
    "\n",
    "        return filtered_tokens\n",
    "\n",
    "    def __call__(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        TfidfVectorizerのtokenizer引数で使用するためのメソッド\n",
    "        LongNameとParentの両方に対応する汎用的なトークン化\n",
    "        \"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return []\n",
    "\n",
    "        text_str = str(text)\n",
    "\n",
    "        # メソッドシグネチャかクラス名かを判定\n",
    "        if '(' in text_str and ')' in text_str:\n",
    "            # メソッドシグネチャとして処理\n",
    "            return self.tokenize_longname(text_str)\n",
    "        else:\n",
    "            # クラス名として処理\n",
    "            return self.tokenize_parent(text_str)\n",
    "\n",
    "\n",
    "class SimplifiedBugHunter:\n",
    "    \"\"\"\n",
    "    カスタムJavaトークナイザー統合版ダウンサンプリング版バグ予測クラス\n",
    "\n",
    "    主な変更点:\n",
    "    - JavaCodeTokenizerを使用したTF-IDF特徴量抽出\n",
    "    - LongNameとParentカラムのJava固有の構造を考慮したトークン化\n",
    "    - CamelCase、snake_case、パッケージ構造の適切な処理\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_selection_threshold: float = 0.001,\n",
    "                 tfidf_max_features: int = 1000,\n",
    "                 java_tokenizer_min_length: int = 2,\n",
    "                 include_package_tokens: bool = False):\n",
    "        \"\"\"\n",
    "        コンストラクタ\n",
    "\n",
    "        Parameters:\n",
    "            feature_selection_threshold (float): 特徴量重要度の閾値\n",
    "            tfidf_max_features (int): TF-IDFで生成する特徴量の最大数\n",
    "            java_tokenizer_min_length (int): Javaトークナイザーの最小トークン長\n",
    "            include_package_tokens (bool): パッケージ名のトークンを含めるかどうか\n",
    "        \"\"\"\n",
    "        self.best_model = None\n",
    "        self.feature_importance = None\n",
    "        self.all_feature_names = None\n",
    "        self.selected_features = None\n",
    "        self.optimization_history = []\n",
    "        self.best_params = None\n",
    "        self.best_loss = float('inf')\n",
    "        self.feature_selection_threshold = feature_selection_threshold\n",
    "        self.initial_X = None\n",
    "        self.initial_y = None\n",
    "        self.tfidf_vectorizer_longname = None\n",
    "        self.tfidf_vectorizer_parent = None\n",
    "        self.tfidf_max_features = tfidf_max_features\n",
    "        self.project_dummies_columns = None\n",
    "        self.scaler = None\n",
    "\n",
    "        # Javaトークナイザーの初期化\n",
    "        self.java_tokenizer = JavaCodeTokenizer(\n",
    "            min_token_length=java_tokenizer_min_length,\n",
    "            include_package_tokens=include_package_tokens\n",
    "        )\n",
    "\n",
    "        # ダウンサンプリング関連の属性\n",
    "        self.downsampled_X = None\n",
    "        self.downsampled_y = None\n",
    "        self.original_class_distribution = None\n",
    "        self.downsampled_train_distribution = None\n",
    "\n",
    "    def read_data(self, data_path: str) -> pd.DataFrame:\n",
    "        \"\"\"データ読み込み\"\"\"\n",
    "        print(\"\\n=== 1) データ読み込み ===\")\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"読み込み完了: {len(df)}行, {len(df.columns)}列\")\n",
    "        return df\n",
    "\n",
    "    def prepare_data(self, data: pd.DataFrame, is_training: bool = True) -> tuple:\n",
    "        \"\"\"データの前処理とラベル作成（カスタムJavaトークナイザー使用）\"\"\"\n",
    "        print(\"\\n=== データ前処理（カスタムJavaトークナイザー + TF-IDF + One-Hot Encoding + 正規化使用）===\")\n",
    "\n",
    "        # ラベル作成（Number of Bugs > 0.5で1、それ以外で0）\n",
    "        y = (data[\"Number of Bugs\"] > 0.5).astype(int) if \"Number of Bugs\" in data.columns and is_training else None\n",
    "\n",
    "        # 元の数値特徴量を選択（Number of Bugsを除く）\n",
    "        numerical_feature_columns = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        numerical_feature_columns = [col for col in numerical_feature_columns if col != \"Number of Bugs\"]\n",
    "        X_numerical = data[numerical_feature_columns].copy()\n",
    "\n",
    "        # 欠損値・無限値の処理 (数値特徴量)\n",
    "        X_numerical = X_numerical.replace([np.inf, -np.inf], np.nan)\n",
    "        X_numerical = X_numerical.fillna(0)\n",
    "\n",
    "        # 正規化処理\n",
    "        if is_training:\n",
    "            self.scaler = StandardScaler()\n",
    "            X_numerical_scaled = self.scaler.fit_transform(X_numerical)\n",
    "        else:\n",
    "            if self.scaler is None:\n",
    "                raise ValueError(\"Scalerが学習されていません。まず訓練データでprepare_dataを実行してください。\")\n",
    "            X_numerical_scaled = self.scaler.transform(X_numerical)\n",
    "        X_numerical_scaled_df = pd.DataFrame(X_numerical_scaled, columns=numerical_feature_columns, index=X_numerical.index)\n",
    "\n",
    "        # LongNameカラムにカスタムJavaトークナイザーを使用したTF-IDFを適用\n",
    "        longname_data = data['LongName'].fillna(\"\").astype(str)\n",
    "        if is_training:\n",
    "            self.tfidf_vectorizer_longname = TfidfVectorizer(\n",
    "                max_features=self.tfidf_max_features,\n",
    "                tokenizer=self.java_tokenizer,\n",
    "                lowercase=False,  # トークナイザー内で小文字化を行うため\n",
    "                token_pattern=None  # カスタムトークナイザーを使用するため\n",
    "            )\n",
    "            X_longname_tfidf = self.tfidf_vectorizer_longname.fit_transform(longname_data)\n",
    "            print(f\"LongName TF-IDF特徴量数: {X_longname_tfidf.shape[1]}\")\n",
    "        else:\n",
    "            if self.tfidf_vectorizer_longname is None:\n",
    "                raise ValueError(\"TF-IDF vectorizer (LongName)が学習されていません。まず訓練データでprepare_dataを実行してください。\")\n",
    "            X_longname_tfidf = self.tfidf_vectorizer_longname.transform(longname_data)\n",
    "        X_longname_tfidf_df = pd.DataFrame(X_longname_tfidf.toarray(),\n",
    "                                            columns=[f'LongName_tfidf_{i}' for i in range(X_longname_tfidf.shape[1])],\n",
    "                                            index=longname_data.index)\n",
    "\n",
    "        # ParentカラムにカスタムJavaトークナイザーを使用したTF-IDFを適用\n",
    "        parent_data = data['Parent'].fillna(\"\").astype(str)\n",
    "        if is_training:\n",
    "            self.tfidf_vectorizer_parent = TfidfVectorizer(\n",
    "                max_features=self.tfidf_max_features,\n",
    "                tokenizer=self.java_tokenizer,\n",
    "                lowercase=False,  # トークナイザー内で小文字化を行うため\n",
    "                token_pattern=None  # カスタムトークナイザーを使用するため\n",
    "            )\n",
    "            X_parent_tfidf = self.tfidf_vectorizer_parent.fit_transform(parent_data)\n",
    "            print(f\"Parent TF-IDF特徴量数: {X_parent_tfidf.shape[1]}\")\n",
    "        else:\n",
    "            if self.tfidf_vectorizer_parent is None:\n",
    "                raise ValueError(\"TF-IDF vectorizer (Parent)が学習されていません。まず訓練データでprepare_dataを実行してください。\")\n",
    "            X_parent_tfidf = self.tfidf_vectorizer_parent.transform(parent_data)\n",
    "        X_parent_tfidf_df = pd.DataFrame(X_parent_tfidf.toarray(),\n",
    "                                          columns=[f'Parent_tfidf_{i}' for i in range(X_parent_tfidf.shape[1])],\n",
    "                                          index=parent_data.index)\n",
    "\n",
    "        # ProjectカラムにOne-Hot Encodingを適用\n",
    "        project_data = data['Project'].fillna(\"Unknown\").astype(str)\n",
    "        X_project_onehot = pd.get_dummies(project_data, prefix='Project', dtype=int)\n",
    "        if is_training:\n",
    "            self.project_dummies_columns = X_project_onehot.columns.tolist()\n",
    "        else:\n",
    "            if self.project_dummies_columns is None:\n",
    "                raise ValueError(\"One-Hot Encodingのカラム情報が学習されていません。まず訓練データでprepare_dataを実行してください。\")\n",
    "            X_project_onehot = X_project_onehot.reindex(columns=self.project_dummies_columns, fill_value=0)\n",
    "\n",
    "        # 全てのXを結合\n",
    "        X = pd.concat([X_numerical_scaled_df, X_longname_tfidf_df, X_parent_tfidf_df, X_project_onehot], axis=1)\n",
    "\n",
    "        if is_training:\n",
    "            self.all_feature_names = X.columns.tolist()\n",
    "            self.initial_X = X\n",
    "            self.initial_y = y\n",
    "\n",
    "            print(f\"初期使用特徴量数 (数値 + Java TF-IDF + One-Hot Encoding): {len(X.columns)}\")\n",
    "            print(f\"  - 数値特徴量: {len(numerical_feature_columns)}\")\n",
    "            print(f\"  - LongName TF-IDF: {X_longname_tfidf.shape[1]}\")\n",
    "            print(f\"  - Parent TF-IDF: {X_parent_tfidf.shape[1]}\")\n",
    "            print(f\"  - Project One-Hot: {len(X_project_onehot.columns)}\")\n",
    "            print(f\"ラベル分布: 0={sum(y==0)}, 1={sum(y==1)}\")\n",
    "\n",
    "            # クラス分布の情報を保存\n",
    "            self.original_class_distribution = {\n",
    "                'class_0': sum(y==0),\n",
    "                'class_1': sum(y==1),\n",
    "                'total': len(y)\n",
    "            }\n",
    "            class_ratio = sum(y==1) / len(y)\n",
    "            print(f\"クラス分布: {class_ratio:.3f} (1の割合)\")\n",
    "\n",
    "            # サンプルトークン化結果の表示\n",
    "            print(\"\\n=== サンプルトークン化結果 ===\")\n",
    "            sample_longname = longname_data.iloc[0] if len(longname_data) > 0 else \"\"\n",
    "            sample_parent = parent_data.iloc[0] if len(parent_data) > 0 else \"\"\n",
    "\n",
    "            if sample_longname:\n",
    "                sample_tokens_longname = self.java_tokenizer(sample_longname)\n",
    "                print(f\"LongName例: {sample_longname}\")\n",
    "                print(f\"→ トークン: {sample_tokens_longname}\")\n",
    "\n",
    "            if sample_parent:\n",
    "                sample_tokens_parent = self.java_tokenizer(sample_parent)\n",
    "                print(f\"Parent例: {sample_parent}\")\n",
    "                print(f\"→ トークン: {sample_tokens_parent}\")\n",
    "        else:\n",
    "            print(f\"予測データの前処理完了: {len(X.columns)}列\")\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def apply_downsampling(self, X: pd.DataFrame, y: pd.Series) -> tuple:\n",
    "        \"\"\"\n",
    "        ダウンサンプリングを適用してクラス不均衡を解決\n",
    "\n",
    "        Parameters:\n",
    "            X (pd.DataFrame): 特徴量データ\n",
    "            y (pd.Series): ラベルデータ\n",
    "\n",
    "        Returns:\n",
    "            tuple: ダウンサンプリング後の(X, y)\n",
    "        \"\"\"\n",
    "        print(\"\\n=== ダウンサンプリング適用 ===\")\n",
    "\n",
    "        # 各クラスのデータを分離\n",
    "        X_class_0 = X[y == 0]\n",
    "        X_class_1 = X[y == 1]\n",
    "        y_class_0 = y[y == 0]\n",
    "        y_class_1 = y[y == 1]\n",
    "\n",
    "        print(f\"ダウンサンプリング前 - クラス0: {len(X_class_0)}件, クラス1: {len(X_class_1)}件\")\n",
    "\n",
    "        # 少数派クラス（クラス1）の数に合わせて多数派クラス（クラス0）をダウンサンプリング\n",
    "        minority_class_size = len(X_class_1)\n",
    "\n",
    "        if len(X_class_0) > minority_class_size:\n",
    "            # ダウンサンプリングを実行\n",
    "            X_class_0_downsampled, y_class_0_downsampled = resample(\n",
    "                X_class_0, y_class_0,\n",
    "                n_samples=minority_class_size,\n",
    "                random_state=GLOBAL_SEED,\n",
    "                replace=False\n",
    "            )\n",
    "\n",
    "            # ダウンサンプリング後のデータを結合\n",
    "            X_balanced = pd.concat([X_class_0_downsampled, X_class_1], axis=0)\n",
    "            y_balanced = pd.concat([y_class_0_downsampled, y_class_1], axis=0)\n",
    "\n",
    "            # インデックスをリセット\n",
    "            X_balanced = X_balanced.reset_index(drop=True)\n",
    "            y_balanced = y_balanced.reset_index(drop=True)\n",
    "\n",
    "            # ランダムにシャッフル\n",
    "            shuffle_indices = np.random.RandomState(GLOBAL_SEED).permutation(len(X_balanced))\n",
    "            X_balanced = X_balanced.iloc[shuffle_indices].reset_index(drop=True)\n",
    "            y_balanced = y_balanced.iloc[shuffle_indices].reset_index(drop=True)\n",
    "\n",
    "            print(f\"ダウンサンプリング後 - クラス0: {len(X_class_0_downsampled)}件, クラス1: {len(X_class_1)}件\")\n",
    "            print(f\"総データ数: {len(X_balanced)}件 (削減率: {(1 - len(X_balanced) / len(X)) * 100:.1f}%)\")\n",
    "\n",
    "        else:\n",
    "            # ダウンサンプリングが不要な場合\n",
    "            X_balanced = X.copy()\n",
    "            y_balanced = y.copy()\n",
    "            print(\"ダウンサンプリング不要: クラス1の方が多いか、同数です\")\n",
    "\n",
    "        # ダウンサンプリング後の訓練データのクラス分布を保存\n",
    "        self.downsampled_train_distribution = {\n",
    "            'class_0': sum(y_balanced == 0),\n",
    "            'class_1': sum(y_balanced == 1),\n",
    "            'total': len(y_balanced)\n",
    "        }\n",
    "\n",
    "        return X_balanced, y_balanced\n",
    "\n",
    "    def log_loss_function(self, y_true: np.ndarray, y_pred_proba: np.ndarray) -> float:\n",
    "        \"\"\"Log Loss損失関数\"\"\"\n",
    "        y_pred_proba_clipped = np.clip(y_pred_proba, 1e-15, 1 - 1e-15)\n",
    "        return log_loss(y_true, y_pred_proba_clipped)\n",
    "\n",
    "    def evaluate_model_with_cv(self, params: dict, X: pd.DataFrame, y: pd.Series,\n",
    "                              k_folds: int = 3) -> float:\n",
    "        \"\"\"交差検証を用いたモデル評価（ダウンサンプリングは外部で適用済み）\"\"\"\n",
    "        try:\n",
    "            # Random Forestモデルの作成（class_weightは削除）\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=int(params['n_estimators']),\n",
    "                max_depth=int(params['max_depth']),\n",
    "                random_state=GLOBAL_SEED,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            # 交差検証でLog Lossを評価\n",
    "            cv = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=GLOBAL_SEED)\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for train_idx, val_idx in cv.split(X, y):\n",
    "                X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "                # 各フォールド内でダウンサンプリングは行わない（すでに外で適用済み）\n",
    "                # ここではX_train_fold, y_train_fold をそのまま使用\n",
    "                rf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "                # 予測確率取得\n",
    "                y_pred_proba = rf.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "                # Log Loss計算\n",
    "                fold_loss = self.log_loss_function(y_val_fold, y_pred_proba)\n",
    "                total_loss += fold_loss\n",
    "\n",
    "            avg_loss = total_loss / k_folds\n",
    "            return avg_loss\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"評価エラー: {e}\")\n",
    "            return float('inf')\n",
    "\n",
    "    def optimize_hyperparameters_with_log_loss(self, X: pd.DataFrame, y: pd.Series,\n",
    "                                                max_iterations: int = 30) -> dict:\n",
    "        \"\"\"Log Loss損失関数を用いたベイジアン最適化（ダウンサンプリングは外部で適用済み）\"\"\"\n",
    "        print(\"\\n=== Log Lossベース ベイジアン最適化（カスタムJavaトークナイザー使用）===\")\n",
    "        print(\"最適化手法: Bayesian Optimization (scikit-optimize)\")\n",
    "        print(\"探索パラメータ: n_estimators, max_depth\")\n",
    "        print(\"クラス不均衡対応: ダウンサンプリング (事前に適用済み)\")\n",
    "        print(\"特徴量: 数値 + Java TF-IDF + One-Hot Encoding + 正規化\")\n",
    "\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_params = None\n",
    "        self.optimization_history = []\n",
    "\n",
    "        # 探索空間の定義\n",
    "        search_space = [\n",
    "            Integer(100, 300, name='n_estimators'),\n",
    "            Integer(10, 20, name='max_depth'),\n",
    "        ]\n",
    "\n",
    "        # 目的関数\n",
    "        def objective(params):\n",
    "            n_estimators, max_depth = params\n",
    "\n",
    "            param_dict = {\n",
    "                'n_estimators': int(n_estimators),\n",
    "                'max_depth': int(max_depth),\n",
    "            }\n",
    "\n",
    "            # モデル評価 (X, y はすでにダウンサンプリング済み)\n",
    "            loss = self.evaluate_model_with_cv(param_dict, X, y)\n",
    "\n",
    "            # 履歴記録\n",
    "            self.optimization_history.append({\n",
    "                'params': param_dict.copy(),\n",
    "                'loss': loss\n",
    "            })\n",
    "\n",
    "            # 最良パラメータ更新\n",
    "            if loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "                self.best_params = param_dict.copy()\n",
    "                print(f\"新しい最良損失: {loss:.4f}\")\n",
    "                print(f\"パラメータ: {param_dict}\")\n",
    "\n",
    "            return loss\n",
    "\n",
    "        # Bayesian Optimization実行\n",
    "        print(\"Bayesian Optimization開始...\")\n",
    "        result = gp_minimize(\n",
    "            func=objective,\n",
    "            dimensions=search_space,\n",
    "            n_calls=max_iterations,\n",
    "            random_state=GLOBAL_SEED,\n",
    "            acq_func='EI',\n",
    "            n_initial_points=5\n",
    "        )\n",
    "\n",
    "        print(f\"\\nBayesian Optimization完了!\")\n",
    "        print(f\"最良損失: {self.best_loss:.4f}\")\n",
    "        final_best_params = self.best_params.copy() if self.best_params else {}\n",
    "        print(f\"最良パラメータ: {final_best_params}\")\n",
    "        print(f\"総評価回数: {len(self.optimization_history)}\")\n",
    "\n",
    "        return final_best_params\n",
    "\n",
    "    def train_initial_model_for_feature_importance(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"特徴量重要度を取得するための初期モデルを学習（ダウンサンプリング済みデータで学習）\"\"\"\n",
    "        print(\"\\n=== 特徴量重要度取得のための初期モデル学習（カスタムJavaトークナイザー版）===\")\n",
    "\n",
    "        # X, y はすでにダウンサンプリング済みとして受け取る\n",
    "        initial_rf = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=GLOBAL_SEED,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        initial_rf.fit(X, y)\n",
    "        self.feature_importance = initial_rf.feature_importances_\n",
    "        print(\"初期モデル学習と特徴量重要度の計算が完了しました。\")\n",
    "        return initial_rf\n",
    "\n",
    "    def select_features_by_importance(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"特徴量重要度に基づいて特徴量を選択\"\"\"\n",
    "        if self.feature_importance is None:\n",
    "            raise ValueError(\"特徴量重要度が計算されていません。まず初期モデルを学習してください。\")\n",
    "\n",
    "        feature_names = self.all_feature_names\n",
    "        if len(feature_names) != len(self.feature_importance):\n",
    "            print(\"警告: 特徴量名と重要度の数が一致しません。現在のXのカラムを再確認します。\")\n",
    "            feature_names = X.columns.tolist()\n",
    "            if len(feature_names) != len(self.feature_importance):\n",
    "                raise ValueError(\"特徴量名と重要度の数が一致しません。\")\n",
    "\n",
    "        # 重要度が閾値以上の特徴量を選択\n",
    "        selected_feature_indices = np.where(self.feature_importance >= self.feature_selection_threshold)[0]\n",
    "        self.selected_features = [feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "        # 選択された特徴量が存在しない場合のエラーハンドリング\n",
    "        if not self.selected_features:\n",
    "            print(f\"警告: 特徴量重要度の閾値 {self.feature_selection_threshold} では、どの特徴量も選択されませんでした。\")\n",
    "            print(\"すべての特徴量を再度含めて処理を続行します。\")\n",
    "            self.selected_features = feature_names\n",
    "            X_selected = X\n",
    "        else:\n",
    "            X_selected = X[self.selected_features]\n",
    "\n",
    "        print(f\"\\n=== 特徴量削減 ===\")\n",
    "        print(f\"特徴量削減前の数: {len(feature_names)}\")\n",
    "        print(f\"選択された特徴量数 (閾値 {self.feature_selection_threshold}): {len(self.selected_features)}\")\n",
    "\n",
    "        # 特徴量タイプ別の統計\n",
    "        longname_count = len([f for f in self.selected_features if f.startswith('LongName_tfidf_')])\n",
    "        parent_count = len([f for f in self.selected_features if f.startswith('Parent_tfidf_')])\n",
    "        project_count = len([f for f in self.selected_features if f.startswith('Project_')])\n",
    "        numerical_count = len(self.selected_features) - longname_count - parent_count - project_count\n",
    "\n",
    "        print(f\"  - 数値特徴量: {numerical_count}\")\n",
    "        print(f\"  - LongName TF-IDF: {longname_count}\")\n",
    "        print(f\"  - Parent TF-IDF: {parent_count}\")\n",
    "        print(f\"  - Project One-Hot: {project_count}\")\n",
    "\n",
    "        return X_selected\n",
    "\n",
    "    def train_optimized_model(self, X: pd.DataFrame, y: pd.Series, optimal_params: dict):\n",
    "        \"\"\"最適化されたパラメータでモデルを学習（ダウンサンプリング済みデータで学習）\"\"\"\n",
    "        print(\"\\n=== 最適化モデル学習（カスタムJavaトークナイザー版）===\")\n",
    "\n",
    "        # X, y はすでにダウンサンプリング済みとして受け取る\n",
    "        rf_params = {\n",
    "            'n_estimators': int(optimal_params['n_estimators']),\n",
    "            'max_depth': int(optimal_params['max_depth']),\n",
    "            'random_state': GLOBAL_SEED,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "\n",
    "        self.best_model = RandomForestClassifier(**rf_params)\n",
    "        self.best_model.fit(X, y)\n",
    "\n",
    "        print(\"最適化モデル学習完了\")\n",
    "        print(f\"最終パラメータ: {rf_params}\")\n",
    "        print(f\"学習データ: {len(X)}件\")\n",
    "\n",
    "        # 特徴量重要度の取得\n",
    "        if hasattr(self.best_model, 'feature_importances_'):\n",
    "            self.feature_importance = self.best_model.feature_importances_\n",
    "            print(\"学習済みモデルから特徴量重要度を計算しました。\")\n",
    "        else:\n",
    "            self.feature_importance = None\n",
    "            print(\"警告: 学習済みモデルは特徴量重要度を提供しません。\")\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def comprehensive_evaluation(self, X_test: pd.DataFrame, y_test: pd.Series) -> dict:\n",
    "        \"\"\"包括的評価（デフォルトしきい値0.5のみ使用）\"\"\"\n",
    "        print(\"\\n=== 包括的評価 ===\")\n",
    "\n",
    "        # 予測確率取得\n",
    "        y_pred_proba = self.best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # デフォルトしきい値（0.5）での予測\n",
    "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "        # 評価指標の計算\n",
    "        results = {\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "            'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_test, y_pred),\n",
    "            'LogLoss': self.log_loss_function(y_test, y_pred_proba),\n",
    "            'Threshold': 0.5\n",
    "        }\n",
    "\n",
    "        print(\"\\n--- 評価結果（しきい値=0.5）---\")\n",
    "        for metric, score in results.items():\n",
    "            if metric != 'Threshold':\n",
    "                print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "        return results, y_pred, y_pred_proba\n",
    "\n",
    "    def run_pipeline(self, data_path: str):\n",
    "        \"\"\"カスタムJavaトークナイザーを組み込んだパイプラインの実行\"\"\"\n",
    "        print(\"=== カスタムJavaトークナイザー版ダウンサンプリングバグ予測パイプライン ===\")\n",
    "\n",
    "        # 1. データ読み込み\n",
    "        data = self.read_data(data_path)\n",
    "\n",
    "        # 2. データ準備（カスタムJavaトークナイザー、TF-IDF特徴量、One-Hot Encoding特徴量、正規化を使用）\n",
    "        X_full, y_full = self.prepare_data(data, is_training=True)\n",
    "\n",
    "        # 3. ダウンサンプリングをここで一度だけ適用\n",
    "        X_downsampled, y_downsampled = self.apply_downsampling(X_full, y_full)\n",
    "        print(f\"ダウンサンプリング後データセットサイズ: {len(X_downsampled)}行\")\n",
    "\n",
    "        # 4. データ分割（ダウンサンプリング後のデータに対して）\n",
    "        X_train_ds, X_test_ds, y_train_ds, y_test_ds = train_test_split(\n",
    "            X_downsampled, y_downsampled, test_size=0.2, random_state=GLOBAL_SEED, stratify=y_downsampled\n",
    "        )\n",
    "        print(f\"訓練データ (ダウンサンプリング後): {len(X_train_ds)}行, テストデータ (ダウンサンプリング後): {len(X_test_ds)}行\")\n",
    "\n",
    "        # 5. 特徴量重要度取得のための初期モデル学習（ダウンサンプリング済み訓練データで学習）\n",
    "        self.train_initial_model_for_feature_importance(X_train_ds, y_train_ds)\n",
    "\n",
    "        # 6. 特徴量削減\n",
    "        X_train_reduced = self.select_features_by_importance(X_train_ds)\n",
    "        X_test_reduced = self.select_features_by_importance(X_test_ds)\n",
    "\n",
    "        # 7. Log Lossベースハイパーパラメータ最適化（削減された訓練データで実施）\n",
    "        optimal_params = self.optimize_hyperparameters_with_log_loss(\n",
    "            X_train_reduced, y_train_ds, max_iterations=30\n",
    "        )\n",
    "\n",
    "        # 8. 最適化モデル学習（削減された訓練データで実施）\n",
    "        optimized_model = self.train_optimized_model(\n",
    "            X_train_reduced, y_train_ds, optimal_params\n",
    "        )\n",
    "\n",
    "        # 9. 評価（削減されたテストデータで実施）\n",
    "        results, y_pred, y_pred_proba = self.comprehensive_evaluation(\n",
    "            X_test_reduced, y_test_ds\n",
    "        )\n",
    "\n",
    "        return results, optimal_params\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> tuple:\n",
    "        \"\"\"予測（カスタムJavaトークナイザー版）\"\"\"\n",
    "        if self.best_model is None:\n",
    "            raise ValueError(\"モデルが学習されていません。まずrun_pipeline()を実行してください。\")\n",
    "        if self.selected_features is None:\n",
    "            raise ValueError(\"特徴量削減が実行されていません。まずrun_pipeline()を実行してください。\")\n",
    "        if self.tfidf_vectorizer_longname is None or self.tfidf_vectorizer_parent is None:\n",
    "            raise ValueError(\"TF-IDF Vectorizerが学習されていません。まずrun_pipeline()を実行してください。\")\n",
    "        if self.project_dummies_columns is None:\n",
    "            raise ValueError(\"One-Hot Encoderが学習されていません。まずrun_pipeline()を実行してください。\")\n",
    "        if self.scaler is None:\n",
    "            raise ValueError(\"Scalerが学習されていません。まずrun_pipeline()を実行してください。\")\n",
    "\n",
    "        # 数値特徴量の準備（学習済みスケーラーを適用）\n",
    "        numerical_feature_columns = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        # predict時には \"Number of Bugs\" カラムが存在しない可能性があるので、存在しない場合は除外しない\n",
    "        if \"Number of Bugs\" in numerical_feature_columns:\n",
    "            numerical_feature_columns.remove(\"Number of Bugs\")\n",
    "\n",
    "        X_numerical = X[numerical_feature_columns].copy()\n",
    "        X_numerical = X_numerical.replace([np.inf, -np.inf], np.nan)\n",
    "        X_numerical = X_numerical.fillna(0)\n",
    "\n",
    "        # 学習済みスケーラーを使用して数値特徴量を変換\n",
    "        X_numerical_scaled = self.scaler.transform(X_numerical)\n",
    "        X_numerical_scaled_df = pd.DataFrame(X_numerical_scaled, columns=numerical_feature_columns, index=X_numerical.index)\n",
    "\n",
    "        # 学習済みTF-IDFベクトライザーを使用してTF-IDF変換を適用\n",
    "        longname_data_pred = X['LongName'].fillna(\"\").astype(str)\n",
    "        X_longname_tfidf_pred = self.tfidf_vectorizer_longname.transform(longname_data_pred)\n",
    "        X_longname_tfidf_df_pred = pd.DataFrame(X_longname_tfidf_pred.toarray(),\n",
    "                                                  columns=[f'LongName_tfidf_{i}' for i in range(X_longname_tfidf_pred.shape[1])],\n",
    "                                                  index=X.index)\n",
    "\n",
    "        parent_data_pred = X['Parent'].fillna(\"\").astype(str)\n",
    "        X_parent_tfidf_pred = self.tfidf_vectorizer_parent.transform(parent_data_pred)\n",
    "        X_parent_tfidf_df_pred = pd.DataFrame(X_parent_tfidf_pred.toarray(),\n",
    "                                                columns=[f'Parent_tfidf_{i}' for i in range(X_parent_tfidf_pred.shape[1])],\n",
    "                                                index=X.index)\n",
    "\n",
    "        # 学習済みOne-Hot Encodingカラムを使用してOne-Hot Encoding変換を適用\n",
    "        project_data_pred = X['Project'].fillna(\"Unknown\").astype(str)\n",
    "        X_project_onehot_pred = pd.get_dummies(project_data_pred, prefix='Project', dtype=int)\n",
    "        # 学習時に存在しなかったプロジェクトは全て0として扱うため、reindexを使用\n",
    "        X_project_onehot_pred = X_project_onehot_pred.reindex(columns=self.project_dummies_columns, fill_value=0)\n",
    "\n",
    "        # 予測用の全特徴量を結合\n",
    "        X_processed_full = pd.concat([X_numerical_scaled_df, X_longname_tfidf_df_pred, X_parent_tfidf_df_pred, X_project_onehot_pred], axis=1)\n",
    "\n",
    "        # 学習時に選択された特徴量のみを選択\n",
    "        # 予測時には、学習時に選択された特徴量のみを使い、存在しない場合は0で埋める\n",
    "        X_processed = pd.DataFrame(0, index=X_processed_full.index, columns=self.selected_features)\n",
    "        for col in self.selected_features:\n",
    "            if col in X_processed_full.columns:\n",
    "                X_processed[col] = X_processed_full[col]\n",
    "\n",
    "        # 予測確率取得\n",
    "        y_pred_proba = self.best_model.predict_proba(X_processed)[:, 1]\n",
    "\n",
    "        # デフォルトしきい値での予測\n",
    "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "        return y_pred, y_pred_proba\n",
    "\n",
    "    def get_feature_analysis(self) -> dict:\n",
    "        \"\"\"特徴量分析結果の取得\"\"\"\n",
    "        params_to_return = self.best_params.copy() if self.best_params else {}\n",
    "\n",
    "        # ダウンサンプリング情報を追加\n",
    "        downsampling_info = {}\n",
    "        if self.original_class_distribution and self.downsampled_train_distribution:\n",
    "            downsampling_info = {\n",
    "                'original_class_0': self.original_class_distribution['class_0'],\n",
    "                'original_class_1': self.original_class_distribution['class_1'],\n",
    "                'original_total': self.original_class_distribution['total'],\n",
    "                'downsampled_train_class_0': self.downsampled_train_distribution['class_0'],\n",
    "                'downsampled_train_class_1': self.downsampled_train_distribution['class_1'],\n",
    "                'downsampled_train_total': self.downsampled_train_distribution['total'],\n",
    "                'reduction_rate': (1 - self.downsampled_train_distribution['total'] / self.original_class_distribution['total']) * 100\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'best_params': params_to_return,\n",
    "            'optimization_history': self.optimization_history,\n",
    "            'feature_importance': self.feature_importance,\n",
    "            'selected_features': self.selected_features,\n",
    "            'all_feature_names': self.all_feature_names,\n",
    "            'feature_selection_threshold': self.feature_selection_threshold,\n",
    "            'tfidf_max_features': self.tfidf_max_features,\n",
    "            'downsampling_info': downsampling_info,\n",
    "            'java_tokenizer_settings': {\n",
    "                'min_token_length': self.java_tokenizer.min_token_length,\n",
    "                'include_package_tokens': self.java_tokenizer.include_package_tokens,\n",
    "                'stopwords_count': len(self.java_tokenizer.java_stopwords)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def display_feature_importance_table(self, top_n: int = 10):\n",
    "        \"\"\"特徴量重要度テーブルの表示\"\"\"\n",
    "        if self.feature_importance is None or self.selected_features is None:\n",
    "            print(\"特徴量重要度は計算されていません。または利用可能なモデルから取得できませんでした。\")\n",
    "            return\n",
    "\n",
    "        feature_names = self.selected_features\n",
    "        if len(feature_names) != len(self.feature_importance):\n",
    "            if self.all_feature_names is not None and len(self.feature_importance) == len(self.all_feature_names):\n",
    "                feature_names = self.all_feature_names\n",
    "            else:\n",
    "                print(\"エラー: 選択された特徴量名と最終モデルの重要度の数が一致しません。\")\n",
    "                return\n",
    "\n",
    "        importance_df = pd.DataFrame({\n",
    "            '特徴量': feature_names,\n",
    "            '重要度': self.feature_importance\n",
    "        }).sort_values('重要度', ascending=False)\n",
    "\n",
    "        # self.selected_features に含まれる特徴量のみをフィルタリング\n",
    "        importance_df_filtered = importance_df[importance_df['特徴量'].isin(self.selected_features)]\n",
    "\n",
    "        # 特徴量タイプの分類\n",
    "        importance_df_filtered['タイプ'] = importance_df_filtered['特徴量'].apply(\n",
    "            lambda x: 'LongName TF-IDF' if x.startswith('LongName_tfidf_')\n",
    "                     else 'Parent TF-IDF' if x.startswith('Parent_tfidf_')\n",
    "                     else 'Project' if x.startswith('Project_')\n",
    "                     else '数値'\n",
    "        )\n",
    "\n",
    "        print(f\"\\n=== 上位{top_n}特徴量重要度 (カスタムJavaトークナイザー版) ===\")\n",
    "        display_df = importance_df_filtered.head(top_n)[['特徴量', 'タイプ', '重要度']].copy()\n",
    "        print(display_df.to_string(index=False))\n",
    "\n",
    "        # タイプ別の統計\n",
    "        print(f\"\\n=== 特徴量タイプ別統計 ===\")\n",
    "        type_stats = importance_df_filtered['タイプ'].value_counts()\n",
    "        for feature_type, count in type_stats.items():\n",
    "            avg_importance = importance_df_filtered[importance_df_filtered['タイプ'] == feature_type]['重要度'].mean()\n",
    "            print(f\"{feature_type}: {count}個 (平均重要度: {avg_importance:.4f})\")\n",
    "\n",
    "        return importance_df_filtered\n",
    "\n",
    "    def display_downsampling_summary(self):\n",
    "        \"\"\"ダウンサンプリングのサマリー表示\"\"\"\n",
    "        if not self.original_class_distribution or not self.downsampled_train_distribution:\n",
    "            print(\"ダウンサンプリング情報がありません。\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n=== ダウンサンプリング サマリー ===\")\n",
    "        print(f\"元データ (全データ):\")\n",
    "        print(f\"  クラス 0: {self.original_class_distribution['class_0']:,}件\")\n",
    "        print(f\"  クラス 1: {self.original_class_distribution['class_1']:,}件\")\n",
    "        print(f\"  合計: {self.original_class_distribution['total']:,}件\")\n",
    "\n",
    "        downsampled_class_0 = self.downsampled_train_distribution['class_0']\n",
    "        downsampled_class_1 = self.downsampled_train_distribution['class_1']\n",
    "        downsampled_total = self.downsampled_train_distribution['total']\n",
    "\n",
    "        print(f\"\\nダウンサンプリング後 (訓練データ):\")\n",
    "        print(f\"  クラス 0: {downsampled_class_0:,}件\")\n",
    "        print(f\"  クラス 1: {downsampled_class_1:,}件\")\n",
    "        print(f\"  合計: {downsampled_total:,}件\")\n",
    "\n",
    "        reduction_rate = (1 - downsampled_total / self.original_class_distribution['total']) * 100\n",
    "        print(f\"\\n元データ全体からの削減率: {reduction_rate:.1f}%\")\n",
    "\n",
    "        original_imbalance = self.original_class_distribution['class_0'] / self.original_class_distribution['class_1'] if self.original_class_distribution['class_1'] > 0 else float('inf')\n",
    "        new_imbalance = downsampled_class_0 / downsampled_class_1 if downsampled_class_1 > 0 else float('inf')\n",
    "        print(f\"クラス不均衡比 (クラス0/クラス1):\")\n",
    "        print(f\"  元データ (全データ): {original_imbalance:.2f}:1\")\n",
    "        print(f\"  ダウンサンプリング後 (訓練データ): {new_imbalance:.2f}:1\")\n",
    "\n",
    "    def display_tokenizer_analysis(self, sample_size: int = 5):\n",
    "        \"\"\"Javaトークナイザーの動作例を表示\"\"\"\n",
    "        if self.initial_X is None:\n",
    "            print(\"データが準備されていません。まずrun_pipeline()を実行してください。\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n=== Javaトークナイザー動作例 ===\")\n",
    "        print(f\"設定:\")\n",
    "        print(f\"  最小トークン長: {self.java_tokenizer.min_token_length}\")\n",
    "        print(f\"  パッケージトークン含む: {self.java_tokenizer.include_package_tokens}\")\n",
    "        print(f\"  ストップワード数: {len(self.java_tokenizer.java_stopwords)}\")\n",
    "\n",
    "        # サンプルの取得と表示\n",
    "        if hasattr(self, 'initial_X'):\n",
    "            # 元データから LongName と Parent を取得\n",
    "            data_sample = pd.read_csv(\"method-p.csv\").head(sample_size)\n",
    "\n",
    "            print(f\"\\n=== LongName トークン化例 (上位{sample_size}件) ===\")\n",
    "            for i, longname in enumerate(data_sample['LongName'].head(sample_size)):\n",
    "                tokens = self.java_tokenizer(str(longname))\n",
    "                print(f\"{i+1}. {longname}\")\n",
    "                print(f\"   → {tokens}\")\n",
    "                if i < sample_size - 1:\n",
    "                    print()\n",
    "\n",
    "            print(f\"\\n=== Parent トークン化例 (上位{sample_size}件) ===\")\n",
    "            for i, parent in enumerate(data_sample['Parent'].head(sample_size)):\n",
    "                tokens = self.java_tokenizer(str(parent))\n",
    "                print(f\"{i+1}. {parent}\")\n",
    "                print(f\"   → {tokens}\")\n",
    "                if i < sample_size - 1:\n",
    "                    print()\n",
    "\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    # CSVファイルパスを指定\n",
    "    data_path = \"method-p.csv\"\n",
    "\n",
    "    # カスタムJavaトークナイザー統合版バグハンターのインスタンス作成\n",
    "    bug_hunter = SimplifiedBugHunter(\n",
    "        feature_selection_threshold=0.001,\n",
    "        tfidf_max_features=1000,\n",
    "        java_tokenizer_min_length=2,\n",
    "        include_package_tokens=False  # パッケージ名を除外して、より重要な部分に集中\n",
    "    )\n",
    "\n",
    "    # パイプライン実行\n",
    "    results, optimal_params = bug_hunter.run_pipeline(data_path)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"カスタムJavaトークナイザー版バグ予測完了!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"F1スコア: {results['F1']:.3f}\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "\n",
    "    # ダウンサンプリングサマリーの表示\n",
    "    bug_hunter.display_downsampling_summary()\n",
    "\n",
    "    # 特徴量重要度テーブルの表示\n",
    "    bug_hunter.display_feature_importance_table(top_n=15)\n",
    "\n",
    "    # トークナイザーの動作例表示\n",
    "    bug_hunter.display_tokenizer_analysis(sample_size=3)\n",
    "\n",
    "    feature_analysis = bug_hunter.get_feature_analysis()\n",
    "    print(f\"\\n最適パラメータ: {feature_analysis['best_params']}\")\n",
    "    print(f\"選択された特徴量数: {len(feature_analysis['selected_features'])}\")\n",
    "    print(f\"全特徴量数 (Java TF-IDF含む): {len(feature_analysis['all_feature_names'])}\")\n",
    "    print(f\"TF-IDF最大特徴量数: {feature_analysis['tfidf_max_features']}\")\n",
    "    print(f\"Javaトークナイザー設定: {feature_analysis['java_tokenizer_settings']}\")\n",
    "\n",
    "    if feature_analysis['downsampling_info']:\n",
    "        ds_info = feature_analysis['downsampling_info']\n",
    "        print(f\"データ削減率: {ds_info['reduction_rate']:.1f}%\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bughunter-FzxI4xcd-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
